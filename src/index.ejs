<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
</head>

<body>

  <d-front-matter>
    <script id="distill-front-matter" type="text/json">
      {
        "title": "Finding Features and Adversarial Inputs for MemoryDT",
        "description": "We analyse the embedding space of a gridworld decision transformer showing that it has developed extensive structure which reflects properties of the model, the gridworld environment and the task. We’re able to extract features and corresponding linear feature representations. Finding that one of these feature representations is present in many different embeddings, we predicted several adversarial inputs  (observations with “distractor” items) that trick the model about what it is seeing. We show that these adversaries work as effectively as changing the feature (in the environment), but that we can also intervene directly on the underlying linear feature representation to achieve the same effects. Whilst methodologically simple, this analysis shows that mechanistic investigation of gridworld models are tractable and may support fundamental mechanistic interpretability research and its application to AI alignment. ",
        "published": "Oct 30, 2023",
        "authors": [
          {
            "author": "Joseph Bloom",
            "authorURL": "https://www.jbloomaus.com/",
            "affiliations": [{ "name": "Independent" }]
          },
          {
            "author": "Jay Bailey",
            "authorURL": "https://www.lesswrong.com/users/jay-bailey",
            "affiliations": [{ "name": "Independent" }]
          }
        ],
        "katex": {
          "delimiters": [{ "left": "$$", "right": "$$", "display": false }]
        }
      }
    </script>
  </d-front-matter>

  <d-title>
    <figure style="grid-column: page; margin: 1rem 0">
      <p>
      <h1>Finding Features and Adversarial Inputs for MemoryDT</h1>
      </p>
    </figure>

  </d-title>

  <d-article>

    <d-contents>
      <nav class="l-text figcaption">
        <h3>Contents</h3>
        <div><a href="#TLDR">TLDR</a></div>
        <div><a href="#Key-Results">Key Results</a></div>
        <ul>
          <li><a href="#Object-Level-Results">Object Level Results</a></li>
          <li><a href="#Broader-Connections">Broader Connections</a></li>
        </ul>
        <div><a href="#Introduction">Introduction</a></div>
        <ul>
          <li><a href="#Why-Decision-Transformers">Why study GridWorld Decision Transformers?</a></li>
          <li><a href="#AI-Alignment-and-the-Linear-Representation-Hypothesis">AI Alignment and the Linear
              Representation
              Hypothesis</a></li>
          <li><a href="#The-MiniGrid-Memory-Task">The MiniGrid Memory Task</a></li>
          <li><a href="#Observation-Embeddings">MemoryDT Observation Embeddings are constructed via a Compositional
              Code. </a></li>
        </ul>
        <div>
          <a href="#Why_study_GridWorld_Decision_Transformers">Why study GridWorld Decision Transformers?</a>
        </div>

      </nav>
    </d-contents>

    <p>
      Code: <a href="https://github.com/jbloomAus/DecisionTransformerInterpretability">repository</a>,
      Model/Training: <a
        href="https://wandb.ai/jbloom/DecisionTransformerInterpretability/reports/A-Mechanistic-Analysis-of-a-GridWorld-Agent-Simulator--Vmlldzo0MzY2OTAy">here</a>.
      Task: <a href="https://minigrid.farama.org/environments/minigrid/MemoryEnv/#memory">here</a>.<br><br>
      <i>
        Epistemic status: I think the basic results are pretty solid, but I’m less sure
        about how these results relate to broader phenomena such as superposition or other
        modalities such as language models.
      </i>
    </p>

    <h2 id="TLDR">TLDR</h2>
    <p>
      We analyse the embedding space of a gridworld decision transformer, showing that it has developed an extensive
      structure that reflects the properties of the model, the gridworld environment and the task. We can identify
      linear feature representations for task-relevant concepts and show the distribution of these features in the
      embedding space. We use these insights to predict several adversarial inputs (observations with “distractor”
      items) that trick the model about what it is seeing. We show that these adversaries work as effectively as
      changing the feature (in the environment). However, we can also intervene directly on the underlying linear
      feature representation to achieve the same effects.
      <strong>
        Whilst methodologically simple, this analysis shows that mechanistic investigation of gridworld models is
        tractable and touches on many different areas of fundamental mechanistic interpretability research and its
        application to AI alignment.
      </strong> <br><br>

      <strong>
        For readers short on time, we recommend reading the following sections:
      </strong>
    <ol>
      <li>Read the Introduction sections on the <a
          href="https://docs.google.com/document/d/1FNd4KbYRRPbs1YVLlsDUGpshZ68AIdDKYyI5uQz3TQM/edit#heading=h.51aamf6oc3nl">task</a>
        and <a
          href="https://docs.google.com/document/d/1FNd4KbYRRPbs1YVLlsDUGpshZ68AIdDKYyI5uQz3TQM/edit#heading=h.tuc933b72sek">observation
          embeddings.</a></li>
      <li>Read the section describing extraction of the <a
          href="https://docs.google.com/document/d/1FNd4KbYRRPbs1YVLlsDUGpshZ68AIdDKYyI5uQz3TQM/edit#heading=h.tkhty7ij90vs">via
          pca.</a></li>
      <li>Read the results sections describing <a
          href="https://docs.google.com/document/d/1FNd4KbYRRPbs1YVLlsDUGpshZ68AIdDKYyI5uQz3TQM/edit#heading=h.rz4da79oapdk">using
          adversaries to change the instruction feature</a> and <a
          href="https://docs.google.com/document/d/1FNd4KbYRRPbs1YVLlsDUGpshZ68AIdDKYyI5uQz3TQM/edit#heading=h.47f7eki4y2s6">comparing
          adversaries to direct intervention.</a></li>
    </ol>
    </p>

    <h2 id="Key-Results">Key Results</h2>
    <h3 id="Object-Level-Results">Object Level Results</h3>
    <ul>
      <li>
        <strong>We show that our observation space has extensive <a href="#geometric_structure">geometric
            structure</a>.</strong>
        <ul>
          <li>We think this structure is induced by properties of experimental set up (partial observations),
            architectural design (compositional embedding schema) and the nature of the specific RL task.
          <li>The learned structure included the use of clustered embeddings and antipodal pairs.
          <li>We see examples of <a href="#isotropic">isotropic</a> and <a
              href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html%23phenomenology-feature-splitting&sa=D&source=editors&ust=1697689300191473&usg=AOvVaw2oF_lthm1whpGnFMS5lA5A">anisotropic
              superposition</a>.</li>
        </ul>
      </li>
      <li>
        <strong>We identify <a href="#linear_feature_representations">interpretable linear feature representations</a>
          in MemoryDT’s observation embedding space.</strong>
        <ul>
          <li>We find that Principal Component Analysis of a subset of embedding vectors produces vectors that linearly
            classify the input space according to task relevant concepts.</li>
          <li>We find that the underlying features appear “smeared” across many embeddings which we interpret as a form
            of equivariance.</li>
        </ul>
      </li>
      <li>
        <strong>We causally validate one of these features, the “<a href="#instruction_feature">instruction feature</a>”
          using <a href="#adversarial_inputs">adversarial inputs/embedding arithmetic</a> and <a
            href="#direct_interventions">direct interventions</a>.</strong>
        <ul>
          <li>It’s easy to break models trained on simple tasks, but our adversaries are targeted, directly flipping the
            models' detection of the learned feature. </li>
          <li>The prediction behind our adversaries also included an arithmetic component which we validated, enabling
            us to relate our results to arithmetic techniques used to generate steering vectors. </li>
          <li>For rigour and completeness, we use direct intervention on the feature to show that it is causal.</li>
          <li>Lastly, we confirm that the adversarial inputs transfer to a different model trained on the same data
            indicating consistent with our adversary working via a feature not a bug.</li>

        </ul>
      </li>
    </ul>

    <h3 id="Broader-Connections">Broader Connections</h3>
    <p>
      While this post summarises relatively few experiments on just one model, we find our results connect with many
      other ideas which go beyond the details of just this model.
    </p>
    <strong>
      <li>
        We observe superposition like geometry in a gridworld model juxtaposing previous results in toy models and
        language models.
      </li>
      <li>
        We show that interpretability techniques can be used to predict effective adversaries that generalise and
        hypothesise possible mechanisms behind adversarial attacks on language models.
      </li>
      <li>
        We add to a body of evidence suggesting that it’s possible to find and intervene on linear feature
        representations, showing that they exist and are causal.
      </li>
      <li>
        We relate our observations to the phenomenology of steering vectors.
      </li>
    </strong>
    <ul>



    </ul>

    <h2 id="Introduction">Introduction</h2>

    <h3 id="Why-Decision-Transformers">Why study GridWorld Decision Transformers?</h3>

    <p><a
        href="https://www.google.com/url?q=https://arxiv.org/abs/2106.01345&sa=D&source=editors&ust=1697689300194242&usg=AOvVaw3VYDT5VZH8BRgQMCFzqqhO">Decision
        Transformers</a> are a form of offline RL (reinforcement learning) which enable us to use Transformers to solve
      traditional RL tasks. While traditional “online” RL trains a model to receive reward by completing a task, offline
      RL is analogous to language model training with the model being rewarded for predicting the next token.</p>

    <p>Decision Transformers are trained on recorded trajectories which are labelled with the reward achieved.
      Reward-to-Go (RTG). RTG is the time-discounted reward stream that the agent should be getting, i.e., if it’s set
      close to 1 then the model will be incentivised to do well, because it will be taking actions consistent with the
      reference class of other agents which got this reward. RTG isn’t critical to this post, but will be discussed in
      more detail in subsequent posts.</p>

    <p>We’re interested in gridworld decision transformers for the following reasons:</p>
    <ol>
      <li>Decision Transformers are smaller/simpler to the language models we want to understand and align. Decision
        Transformers are transformers, the training trajectories operate a lot like a training corpus and RIG works a
        lot like an instruction/goal prompting. It may be the case that various phenomena associated with <a
          href="https://www.google.com/url?q=https://openai.com/research/gpt-4&sa=D&source=editors&ust=1697689300195010&usg=AOvVaw0cOwkTfQ2FF2BY2iKlSTaE">large
          language models</a> are also present in these models and can be studied.</li>
      <li>We might be able to study alignment relevant phenomena in decision transformers. Previous work has studied
        alignment relevant phenomena (such as goal misgeneralization) in the <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/1711.09883&sa=D&source=editors&ust=1697689300195324&usg=AOvVaw2Cgs54NZajORflqbZ6-FFJ">absence
          of interpretability</a>, or with <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network&sa=D&source=editors&ust=1697689300195503&usg=AOvVaw2GTpjCdoks_iBJrYtkRZ4z">non-transformer</a>
        <a href="https://distill.pub/2020/understanding-rl-vision">architectures</a>. Decision transformers are more
        analogous to pre-trained language models or instruction-tuned language models by default, but we could
        conceivably train them with online learning analogous to RLHF.
      </li>
      <li>We’re working with gridworld tasks because they’re simpler and easier to write. Gridworld RL tasks have been
        used to study <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/1711.09883&sa=D&source=editors&ust=1697689300195875&usg=AOvVaw3zjubmq_HFiS0v5nxOt6L7">alignment
          relevant properties</a> in the past and we’re able to avoid training convolutional layers to process images
        which speeds up training.</li>
    </ol>


    <h3>AI Alignment and the Linear Representation Hypothesis</h3>

    <p>The <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html%23motivation&sa=D&source=editors&ust=1697689300196253&usg=AOvVaw0-Q3o4WFOncfqXBP_tHyKD">linear
        representation hypothesis</a> proposes that the neural networks represent features of the input as directions in
      latent space.</p>

    <p>This post focuses on linear representations for 3 reasons:</p>
    <ol>
      <li><strong>The Linear Representation Hypothesis seems likely to be true.</strong> Evidence on many fronts suggest
        that <a
          href="https://www.google.com/url?q=https://www.beren.io/2023-04-04-DL-models-are-secretly-linear/&sa=D&source=editors&ust=1697689300196673&usg=AOvVaw2DuQx2mOm9nizxSiWSXdLB"></a>some
        version of the linear representation hypothesis holds</a>. Also, <a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/2309.08600.pdf&sa=D&source=editors&ust=1697689300196783&usg=AOvVaw2xj4qsOSo6s6gGkCXptDcL">recent</a>
        <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&sa=D&source=editors&ust=1697689300196917&usg=AOvVaw1odu0hp6ZcaciNVPHNhpdI">publications</a>
        show evidence that is possible to find and interpret linear representations in the residual stream. Therefore,
        it's likely that MemoryDT and other gridworld / decision transformers will make use of linear representations.
      </li>

      <li><strong>The Linear Representation Hypothesis seems likely to be useful.</strong> If the linear representation
        hypothesis is true and we're able to find the corresponding directions in deep neural networks, then we may be
        able to read the thoughts of AI systems directly. Such a feat would not only be step one in <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget&sa=D&source=editors&ust=1697689300197401&usg=AOvVaw370vU5dqaa2lncLV73BGwI">retargeting
          the search</a>, but a huge win for interpretability and many other alignment agendas. Showing that we can
        retarget the search on MemoryDT is one of various win-scenarios for our work.</li>

      <li>Our results seem interesting from the perspective of <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html&sa=D&source=editors&ust=1697689300197639&usg=AOvVaw3lX3jsHvBw1h1pOJ1x5OIR">superposition</a>,
        a phenomena which represents a significant obstacle in interpretability. Previously, it was thought that finding
        meaningful directions in a residual stream would be very difficult due to superposition / entanglement (the
        property whereby linear features are represented in shared dimensions). Results from recent work with sparse
        autoencoders found interpretable features which clump together in groups (<strong>anisotropic
          superposition</strong>) as opposed to repelling and spreading as far as possible (<strong>isotropic
          superposition</strong>).</li>
    </ol>

    <figure>
      <img src="images/image19.png" alt="Anisotropic Superposition">
      <figcaption>
        <p>Diagram from Towards Monosemanticity: Decomposing Language Models With Dictionary Learning </p>
      </figcaption>
    </figure>

    <h3 id="The-MiniGrid-Memory-Task">The MiniGrid Memory Task</h3>

    <p><a
        href="https://www.google.com/url?q=https://www.lesswrong.com/posts/JvQWbrbPjuvw4eqxv/a-mechanistic-interpretability-analysis-of-a-gridworld-agent&sa=D&source=editors&ust=1697689300198182&usg=AOvVaw0rmLWFVNUPq7ysl7K-6yZY">MemoryDT</a>
      is trained to predict actions in trajectories produced by a policy that solves the <a
        href="https://www.google.com/url?q=https://minigrid.farama.org/index.html&sa=D&source=editors&ust=1697689300198327&usg=AOvVaw1fPFZFyT1I8aBTLPV9PHE4">MiniGrid</a>
      Memory task. In this task, the agent is spawned next to an object (a ball or a key) and is rewarded for walking to
      the matching object at the end of the corridor. Due to partial observability, the instruction can’t be seen
      without facing it.
    </p>

    <p><strong>Figure 1</strong> shows all 4 variations on the environment. Please note: </p>

    <ul>
      <li><strong>The agent is always implicitly present at position (3,6), centred at the bottom of the image.
        </strong></li>
      <li><strong>
          The action space is made up of the actions “Left”, “Right”, and “Forward”, along with four other actions not
          useful in this environment.
        </strong></li>
      <li>
        <strong>MemoryDT receives its observations in blocks of three tokens (R, S, A),
        </strong> with the action produced by the model and the Reward-to-Go and the next state/observation provided by
        the environment.
      </li>
    </ul>

    <figure>
      <div id="46a64ae5-2c2c-4a65-ab24-79ed88497949" class="plotly-graph-div" style="height:100%; width:1200px;"></div>
      <script type="text/javascript">                                    window.PLOTLYENV = window.PLOTLYENV || {}; if (document.getElementById("46a64ae5-2c2c-4a65-ab24-79ed88497949")) {
          Plotly.newPlot("46a64ae5-2c2c-4a65-ab24-79ed88497949", [{ "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>", "name": "0", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFMUlEQVR4Xu3c4W3iWBhG4ZdVyoDUEdIHWwENhKkj0MBUkPQBqSPQR/YHInNzZ4GJwXCYex7xw/oGWwgdXRtP5MF0Oo2E1XegHv8wj3/YP/VAIjFQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMV2qDvx+dJp3AFFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqtLt60IOfP3/Wo/OZTqce/4ALHL8enZUrqNBIgQ6T5+Rj93pPnpNh/S41BRPoJFkms2IySmbJ0kabxgh0u3aO6nGSjJJlPVM7GIE+7alza5Q81TOd00Pykix3F1fL5CV5qN91FYxAyzP7PBkkg+S1GE6KbZ3RQ/KerJJJMt4Nx8kkWSXv18+UEWjpx26jrPbzu9MZPSWrY+eu1ZVPX7xAdRlPybye/b/5NRvlBfqSJBl+/fpWxbZO9/DHdW7Nr3auZwRaflmT5CNZf73uLK9HdbrtKvAtHXY5B0agi2Rdz35ZJ4t6pu4eDl537jO6ziLKCHSTPO5pdJ081jOdpPz1+S2ddzwBI9DsGq0ujGbJfbL5OtSJOv/PXOcdT4AJNMmmuMe05Zm9D53v2XXe8QSkQKXfGGh7Ot+z67zjCQy0PZ2v6TvveAJGoJ9/A/pR/4vO71u36EuddzwBI1Bd0tueO3qHrZO3enYBjEC3f760fekC/q0Hx3XY5RwYgerC3r551312neUzBtquxR83OrvmDWkDbdgiGR+8Hl0n42vWGQNt3Vtyn4yT1+I25yp5TcbJ/dXO7J8u8eCG47y7dF1vV/sNdJQrqNAYK6h3l7SHK6jQDFRoBiq0Qd+Pz5NOcZf+n99Zj26N389hvX4/nuKFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0nw8qtL/h+aB9f/5bP349OrdeP7+neKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs3ngwrN54MecYHnd/Z9/Hp0br1+fk/xQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVms8HFZrPBz3iAs/v7Pv49ejcev38nuKF1lKgw+Q5+di93pPnZFi/SyjNBDpJlsmsmIySWbK0UbQ2At2unaN6nCSjZFnPxNFGoE976twaJU/1TBBtBFqe2efJIBkkr8VwUmyLpI1ASz92G2W142JbJO0FqpvSXqAvSZJhMi+Gq2JbJG0EWrY4ST6S9dfrzvJ6VCRtBLpI1vXsl3WyqGeCaCPQTfK4p9F18ljPxNFGoNk1Wp7rk8yS+2TzdSiSZgJNsinuMW15ZsdrKVDdIAMVmoEK7a4e/JU+6oFuhSuo0NpYQQfFtqvpTXEFFZqBCs1AhWagQmvjR5I/jG6WK6jQ2lhBy9tMuimuoEIzUKEZqND+Axb7wb5luWP8AAAAAElFTkSuQmCC", "xaxis": "x", "yaxis": "y", "type": "image" }, { "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>", "name": "1", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFOUlEQVR4Xu3c71HiWhyH8S93LAOsQ+yDWwENyNYhNGAF2gdYh9iH9wWDezxz+SNrch42z2d8kfmtyTDs40mITkbz+TwSVteBevzjPP5x/9QDicRAhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaKOuH58n/QlXUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQrupBx14enqqRz9nPp97/CN6OH49+lGuoEIzUO2Nk8fkY//1ljwm4/q7emagSpLMknWyKCaTZJGsGzdqoNqvnZN6nCSTZF3P+sQI9C55Ttb7k8s6eU7u6u9SVx4O1LkzSR7qWW9aB3qXvCWbZJZM98NpMks2yZuZ9qI8sy+TUTJKXorhrNjuV9NAH5LNqZ/dTcsf3yH6td8oq/1cO3rXLtCHZFnP/t/SRoerUaB3Z9e5s/Rc35fnJMn463/QptjuV6NAd+/Ct1ywi85UtjhLPpLt1+vO8nq0Xy0CvTt63XnIxEW0M6tkW89+2yaretabFoGWV9/fcvGOOu49uT/Q6Da5r2d9ahHoxb+ZuHhHnbRrtPpgsEhuk/evw361CPTiexYX76hzvBf3mHbandk/tQhUOluLQC++Z3HxjrpaLQK9+Jrm4h11tVoE+q1b9KWLd9Rxn38D+lH/S3MtAn09cEfjuG3yWs/012sRaJJ/68FpF+yiM+3+fGn3BdMo0Ndv3nVfuHwOVKNAk6zObnSBuCGnJtoFmmSVTI9ej26TqXUOWtNAk7wmt8k0eSluc26Sl2Sa3HpmH7o+Htxw2qufgZri3V361HoFlY5irKBqi3d36ZMrqNAMVGgGKrRR14/Pk/7ETbp/fmc9uja+P8d1+v54iheagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNB8PqjQ/obng3b9+q/9+PXop3X6+j3FC81AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaD4fVGg+H/SEHp7f2fXx69FP6/T1e4oXmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQfD6o0Hw+6Ak9PL+z6+PXo5/W6ev3FC+0IQU6Th6Tj/3XW/KYjOvvEspgAp0l62RRTCbJIlnbKNowAt2tnZN6nCSTZF3PxDGMQB8O1LkzSR7qmSCGEWh5Zl8mo2SUvBTDWbEtkmEEWvq13yirnRbbIhleoLoqwwv0OUkyTpbFcFNsi2QYgZYtzpKPZPv1urO8HhXJMAJdJdt69ts2WdUzQQwj0Pfk/kCj2+S+noljGIFm32h5rk+ySG6T969DkQwm0CTvxT2mHc/seEMKVFfIQIVmoEK7qQd/pY96oGvhCiq0Yaygo2Lb1fSquIIKzUCFZqBCM1ChDeNDkh+MrpYrqNCGsYKWt5l0VVxBhWagQjNQof0Hv4LBvuJ54cIAAAAASUVORK5CYII=", "xaxis": "x2", "yaxis": "y2", "type": "image" }, { "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>", "name": "2", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFYElEQVR4Xu3c71HiWhyH8a93LEOsA7YPrIAGYOtYaMAKsA+gjsU+vC8YvPHclT9Zk/MQn8/4wvnpyTDuYxKPbu5ms1kkrK4D9finefzT/ikHEomBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0O66fnye9Dc8gwrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFdp9OejA8/NzOfo6s9nM45/Qw/HL0ZfyDCo0A9XRQ/IreTu+/U5+JQ/lZ/XMQJUkmSabZNGYjJJFsqncqIHqeO4cleMkGSWbctYnRqDjZJ1sjheXTbJOxuVnqSvzT+o8GCXzctab2oGOk9/JNpkmk+NwkkyTbfLbTHvRvLIvk7vkLnlpDKeN9/tVNdB5sj33vbut+e37Hf08vtOs9v3c0bt6gc6TZTn7s6WNfl+VAh1fXOfB0mt9X9ZJkoeP/0Dbxvv9qhTo4atwlRZLdKFmi9PkLdl/vO9s3o/2q0ag45P3nZ8ZeRLtzCrZl7P/7JNVOetNjUCbd99Xab1Qp70mPz5pdJ/8KGd9qhFo699MtF6osw6NFj8YLJLH5PXjsF81Am29Z9F6oS7x2thjOqh3ZX9XI1DpYjUCbb1n0XqhblaNQFvf07ReqJtVI9CrtuibWi/Uae9/A/pWfqS6GoHuPtnROG2f7MqZBq9GoEmeysF5LZboQoc/Xzq8wVQKdHflrvvC0+c3VSnQJKuLG10gNuRURb1Ak6ySycn70X0ysc5vrWqgSXbJYzJJXhrbnNvkJZkkj17Zv7s+Htxw3s6fgari7S69q30GlU5inEFVF2936Z1nUKEZqNAMVGh3XT8+T/ob9+n++Z3l6Nb49Tmt06+Pl3ihGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrN54MKbQjPB+369d/68cvRV+v09XuJF5qBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0Hw+qNB8PugZPTy/s+vjl6Ov1unr9xIvNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1Ch+XxQofl80DN6eH5n18cvR1+t09fvJV5oBio0AxWagQrtewQ6TtbJJnlL3pJNsk7G5WcJ6L4cDMwhzdHH4SRJMk32yVOy+/hRkQz6DDpPtv+rs2mUbJN5ORbHcAOdJ8ty9mdLG+UaaKDji+s8WHpLCjXQQNfl4LwWS9S9IQY6Pnnf+ZmRJ1GiIQa6KAeXar1QnRlioA/l4FKtF6ozQwz0sM3ZQuuF6swQA9WADDHQbTm4VOuF6swQA30tB5dqvVCdGWKgV23RN7VeqM4MMdBdsi9n5+39qxGiIQaa5KkcnNdiibo30EB3V+66Lzx9Qg000CSrixtdJKtyJojhBppklUxO3o/uk4l1og060CS75DGZJC+Nbc5t8pJMkkev7HRD/y8fBzt/BrpVQz+D6sYZqNAMVGgGKjQDFdq/ORrI24OGp80AAAAASUVORK5CYII=", "xaxis": "x3", "yaxis": "y3", "type": "image" }, { "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>", "name": "3", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFWUlEQVR4Xu3c71HiahxH8a93LEOsA7YPrIAG4Nax0IAVYB9AHWIf3hcM3vjs8i8h5BjOZ3zh/PTJMMyZJD7L5mEymUTCajtQj3+cxz/un3IgkRio0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEK7aHtx+dJTXgGFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqtMdy0ILX19dydD2TycTjH3GD45ejq/IMKjRSoE/J7+Rz//We/E6eyt/SXcEEOk5WyawyGSSzZGWjd40R6O7cOSjHSTJIVuVM94MR6PRAnTuDZFrOdE3DZJms9jdXq2SZDMvf6gQj0OqVfZ48JA/JW2U4rnyvKxom78k6GSej/XCUjJN18t59poxAq/7df1Ot9uu90xVNk/Wpa9e648sXL1DdxjSZl7O/m3fZKC/QZZLk6fvbt658r+aGZ9e5M+/sWs8ItPpmjZPPZPv9vrN6P6rmdmeBi9RYcg2MQBfJtpz9b5ssypnqGx697zxk0M1JlBHoR/LrQKPb5Fc5UyPVvz4vUnthA4xAs2+0uDGaJc/Jx/ehGqr9L3O1FzaACTTJR2WPaccrextq79nVXtgAKVDpDwZ6f2rv2dVe2ICB3p/a9/S1FzbACPTrM6Cf5U90fRdt0VfVXtgAI1Dd0ubAjt5x22RTzm6AEeju40u7L93ASzk4rcaSa2AEqhvbXLjrPuvm9BkDvV+LsxuddbkhbaB3bJGMjt6PbpNRl3XGQO/dJnlORslbZZtznbwlo+S5syv7l1s8uOE0d5e6tensb6CTPIMKjXEGdXdJB3gGFZqBCs1AhfbQ9uPzpCYe0/7zO8vRT+P7c1yr74+XeKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs3ngwqtD88Hbfv1//Tjl6Nra/X1e4kXmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQfD6o0Hw+6Ak3eH5n28cvR9fW6uv3Ei80AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKH5fFCh+XzQE27w/M62j1+Orq3V1+8lXmgGKjQDFZqBCu0+Ah0my2SVfCafySpZJsPytwT0WA56Zpfm4PtwlCQZJ9vkJdl8/6lIen0GnSbrP+qsGiTrZFqOxdHfQKfJvJz93dxGuXoa6PDsOnfm3pJC9TTQZTk4rcYSta+PgQ6P3nceMvAkStTHQGfl4Fy1F6o1fQz0qRycq/ZCtaaPge62OWuovVCt6WOg6pE+BrouB+eqvVCt6WOgH+XgXLUXqjV9DPSiLfqq2gvVmj4Gukm25ey0rZ8aIepjoEleysFpNZaofT0NdHPhrvvM0ydUTwNNsji70VmyKGeC6G+gSRbJ6Oj96DYZWSdarwNNskmek1HyVtnmXCdvySh59spO1/f/8rGz8W+gn6rvZ1D9cAYqNAMVmoEKzUCF9h+QhMjbNytVWQAAAABJRU5ErkJggg==", "xaxis": "x4", "yaxis": "y4", "type": "image" }], { "annotations": [{ "showarrow": false, "text": "Key, Key-Ball", "x": 0.1175, "xanchor": "center", "xref": "paper", "y": 1.0, "yanchor": "bottom", "yref": "paper" }, { "showarrow": false, "text": "Key, Ball-Key", "x": 0.3725, "xanchor": "center", "xref": "paper", "y": 1.0, "yanchor": "bottom", "yref": "paper" }, { "showarrow": false, "text": "Ball, Ball-Key", "x": 0.6275, "xanchor": "center", "xref": "paper", "y": 1.0, "yanchor": "bottom", "yref": "paper" }, { "showarrow": false, "text": "Ball, Key-Ball", "x": 0.8824999999999998, "xanchor": "center", "xref": "paper", "y": 1.0, "yanchor": "bottom", "yref": "paper" }], "font": { "size": 24 }, "margin": { "t": 60 }, "sliders": [{ "active": 0, "currentvalue": { "prefix": "animation_frame=" }, "len": 0.9, "pad": { "b": 10, "t": 60 }, "steps": [{ "args": [["0"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "0", "method": "animate" }, { "args": [["1"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "1", "method": "animate" }, { "args": [["2"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "2", "method": "animate" }, { "args": [["3"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "3", "method": "animate" }, { "args": [["4"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "4", "method": "animate" }, { "args": [["5"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "5", "method": "animate" }, { "args": [["6"], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "6", "method": "animate" }], "x": 0.1, "xanchor": "left", "y": 0, "yanchor": "top" }], "template": { "data": { "candlestick": [{ "decreasing": { "line": { "color": "#000033" } }, "increasing": { "line": { "color": "#000032" } }, "type": "candlestick" }], "contourcarpet": [{ "colorscale": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]], "type": "contourcarpet" }], "contour": [{ "colorscale": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]], "type": "contour" }], "heatmap": [{ "colorscale": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]], "type": "heatmap" }], "histogram2d": [{ "colorscale": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]], "type": "histogram2d" }], "icicle": [{ "textfont": { "color": "white" }, "type": "icicle" }], "sankey": [{ "textfont": { "color": "#000036" }, "type": "sankey" }], "scatter": [{ "marker": { "line": { "width": 0 } }, "type": "scatter" }], "table": [{ "cells": { "fill": { "color": "#000038" }, "font": { "color": "#000037" }, "line": { "color": "#000039" } }, "header": { "fill": { "color": "#000040" }, "font": { "color": "#000036" }, "line": { "color": "#000039" } }, "type": "table" }], "waterfall": [{ "connector": { "line": { "color": "#000036", "width": 2 } }, "decreasing": { "marker": { "color": "#000033" } }, "increasing": { "marker": { "color": "#000032" } }, "totals": { "marker": { "color": "#000034" } }, "type": "waterfall" }] }, "layout": { "coloraxis": { "colorscale": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]] }, "colorscale": { "diverging": [[0.0, "#000021"], [0.1, "#000022"], [0.2, "#000023"], [0.3, "#000024"], [0.4, "#000025"], [0.5, "#000026"], [0.6, "#000027"], [0.7, "#000028"], [0.8, "#000029"], [0.9, "#000030"], [1.0, "#000031"]], "sequential": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]], "sequentialminus": [[0.0, "#000011"], [0.1111111111111111, "#000012"], [0.2222222222222222, "#000013"], [0.3333333333333333, "#000014"], [0.4444444444444444, "#000015"], [0.5555555555555556, "#000016"], [0.6666666666666666, "#000017"], [0.7777777777777778, "#000018"], [0.8888888888888888, "#000019"], [1.0, "#000020"]] }, "colorway": ["#000001", "#000002", "#000003", "#000004", "#000005", "#000006", "#000007", "#000008", "#000009", "#000010"] } }, "updatemenus": [{ "buttons": [{ "args": [null, { "frame": { "duration": 500, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 500, "easing": "linear" } }], "label": "&#9654;", "method": "animate" }, { "args": [[null], { "frame": { "duration": 0, "redraw": true }, "mode": "immediate", "fromcurrent": true, "transition": { "duration": 0, "easing": "linear" } }], "label": "&#9724;", "method": "animate" }], "direction": "left", "pad": { "r": 10, "t": 70 }, "showactive": false, "type": "buttons", "x": 0.1, "xanchor": "right", "y": 0, "yanchor": "top" }], "width": 1200, "xaxis": { "anchor": "y", "domain": [0.0, 0.235], "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "yaxis": { "anchor": "x", "domain": [0.0, 1.0], "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "xaxis2": { "anchor": "y2", "domain": [0.255, 0.49], "matches": "x", "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "yaxis2": { "anchor": "x2", "domain": [0.0, 1.0], "matches": "y", "showticklabels": false, "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "xaxis3": { "anchor": "y3", "domain": [0.51, 0.745], "matches": "x", "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "yaxis3": { "anchor": "x3", "domain": [0.0, 1.0], "matches": "y", "showticklabels": false, "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "xaxis4": { "anchor": "y4", "domain": [0.7649999999999999, 0.9999999999999999], "matches": "x", "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] }, "yaxis4": { "anchor": "x4", "domain": [0.0, 1.0], "matches": "y", "showticklabels": false, "tickmode": "array", "ticktext": ["0", "1", "2", "3", "4", "5", "6"], "tickvals": [16.0, 48.0, 80.0, 112.0, 144.0, 176.0, 208.0] } }, { "responsive": true }).then(function () {
            Plotly.addFrames('46a64ae5-2c2c-4a65-ab24-79ed88497949', [{ "data": [{ "name": "0", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFMUlEQVR4Xu3c4W3iWBhG4ZdVyoDUEdIHWwENhKkj0MBUkPQBqSPQR/YHInNzZ4GJwXCYex7xw/oGWwgdXRtP5MF0Oo2E1XegHv8wj3/YP/VAIjFQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMV2qDvx+dJp3AFFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqtLt60IOfP3/Wo/OZTqce/4ALHL8enZUrqNBIgQ6T5+Rj93pPnpNh/S41BRPoJFkms2IySmbJ0kabxgh0u3aO6nGSjJJlPVM7GIE+7alza5Q81TOd00Pykix3F1fL5CV5qN91FYxAyzP7PBkkg+S1GE6KbZ3RQ/KerJJJMt4Nx8kkWSXv18+UEWjpx26jrPbzu9MZPSWrY+eu1ZVPX7xAdRlPybye/b/5NRvlBfqSJBl+/fpWxbZO9/DHdW7Nr3auZwRaflmT5CNZf73uLK9HdbrtKvAtHXY5B0agi2Rdz35ZJ4t6pu4eDl537jO6ziLKCHSTPO5pdJ081jOdpPz1+S2ddzwBI9DsGq0ujGbJfbL5OtSJOv/PXOcdT4AJNMmmuMe05Zm9D53v2XXe8QSkQKXfGGh7Ot+z67zjCQy0PZ2v6TvveAJGoJ9/A/pR/4vO71u36EuddzwBI1Bd0tueO3qHrZO3enYBjEC3f760fekC/q0Hx3XY5RwYgerC3r551312neUzBtquxR83OrvmDWkDbdgiGR+8Hl0n42vWGQNt3Vtyn4yT1+I25yp5TcbJ/dXO7J8u8eCG47y7dF1vV/sNdJQrqNAYK6h3l7SHK6jQDFRoBiq0Qd+Pz5NOcZf+n99Zj26N389hvX4/nuKFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0nw8qtL/h+aB9f/5bP349OrdeP7+neKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs3ngwrN54MecYHnd/Z9/Hp0br1+fk/xQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVms8HFZrPBz3iAs/v7Pv49ejcev38nuKF1lKgw+Q5+di93pPnZFi/SyjNBDpJlsmsmIySWbK0UbQ2At2unaN6nCSjZFnPxNFGoE976twaJU/1TBBtBFqe2efJIBkkr8VwUmyLpI1ASz92G2W142JbJO0FqpvSXqAvSZJhMi+Gq2JbJG0EWrY4ST6S9dfrzvJ6VCRtBLpI1vXsl3WyqGeCaCPQTfK4p9F18ljPxNFGoNk1Wp7rk8yS+2TzdSiSZgJNsinuMW15ZsdrKVDdIAMVmoEK7a4e/JU+6oFuhSuo0NpYQQfFtqvpTXEFFZqBCs1AhWagQmvjR5I/jG6WK6jQ2lhBy9tMuimuoEIzUKEZqND+Axb7wb5luWP8AAAAAElFTkSuQmCC", "type": "image" }, { "name": "1", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFOUlEQVR4Xu3c71HiWhyH8S93LAOsQ+yDWwENyNYhNGAF2gdYh9iH9wWDezxz+SNrch42z2d8kfmtyTDs40mITkbz+TwSVteBevzjPP5x/9QDicRAhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaKOuH58n/QlXUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQrupBx14enqqRz9nPp97/CN6OH49+lGuoEIzUO2Nk8fkY//1ljwm4/q7emagSpLMknWyKCaTZJGsGzdqoNqvnZN6nCSTZF3P+sQI9C55Ttb7k8s6eU7u6u9SVx4O1LkzSR7qWW9aB3qXvCWbZJZM98NpMks2yZuZ9qI8sy+TUTJKXorhrNjuV9NAH5LNqZ/dTcsf3yH6td8oq/1cO3rXLtCHZFnP/t/SRoerUaB3Z9e5s/Rc35fnJMn463/QptjuV6NAd+/Ct1ywi85UtjhLPpLt1+vO8nq0Xy0CvTt63XnIxEW0M6tkW89+2yaretabFoGWV9/fcvGOOu49uT/Q6Da5r2d9ahHoxb+ZuHhHnbRrtPpgsEhuk/evw361CPTiexYX76hzvBf3mHbandk/tQhUOluLQC++Z3HxjrpaLQK9+Jrm4h11tVoE+q1b9KWLd9Rxn38D+lH/S3MtAn09cEfjuG3yWs/012sRaJJ/68FpF+yiM+3+fGn3BdMo0Ndv3nVfuHwOVKNAk6zObnSBuCGnJtoFmmSVTI9ej26TqXUOWtNAk7wmt8k0eSluc26Sl2Sa3HpmH7o+Htxw2qufgZri3V361HoFlY5irKBqi3d36ZMrqNAMVGgGKrRR14/Pk/7ETbp/fmc9uja+P8d1+v54iheagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNB8PqjQ/obng3b9+q/9+PXop3X6+j3FC81AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaD4fVGg+H/SEHp7f2fXx69FP6/T1e4oXmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQfD6o0Hw+6Ak9PL+z6+PXo5/W6ev3FC+0IQU6Th6Tj/3XW/KYjOvvEspgAp0l62RRTCbJIlnbKNowAt2tnZN6nCSTZF3PxDGMQB8O1LkzSR7qmSCGEWh5Zl8mo2SUvBTDWbEtkmEEWvq13yirnRbbIhleoLoqwwv0OUkyTpbFcFNsi2QYgZYtzpKPZPv1urO8HhXJMAJdJdt69ts2WdUzQQwj0Pfk/kCj2+S+noljGIFm32h5rk+ySG6T969DkQwm0CTvxT2mHc/seEMKVFfIQIVmoEK7qQd/pY96oGvhCiq0Yaygo2Lb1fSquIIKzUCFZqBCM1ChDeNDkh+MrpYrqNCGsYKWt5l0VVxBhWagQjNQof0Hv4LBvuJ54cIAAAAASUVORK5CYII=", "type": "image" }, { "name": "2", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFYElEQVR4Xu3c71HiWhyH8a93LEOsA7YPrIAGYOtYaMAKsA+gjsU+vC8YvPHclT9Zk/MQn8/4wvnpyTDuYxKPbu5ms1kkrK4D9finefzT/ikHEomBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0O66fnye9Dc8gwrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFdp9OejA8/NzOfo6s9nM45/Qw/HL0ZfyDCo0A9XRQ/IreTu+/U5+JQ/lZ/XMQJUkmSabZNGYjJJFsqncqIHqeO4cleMkGSWbctYnRqDjZJ1sjheXTbJOxuVnqSvzT+o8GCXzctab2oGOk9/JNpkmk+NwkkyTbfLbTHvRvLIvk7vkLnlpDKeN9/tVNdB5sj33vbut+e37Hf08vtOs9v3c0bt6gc6TZTn7s6WNfl+VAh1fXOfB0mt9X9ZJkoeP/0Dbxvv9qhTo4atwlRZLdKFmi9PkLdl/vO9s3o/2q0ag45P3nZ8ZeRLtzCrZl7P/7JNVOetNjUCbd99Xab1Qp70mPz5pdJ/8KGd9qhFo699MtF6osw6NFj8YLJLH5PXjsF81Am29Z9F6oS7x2thjOqh3ZX9XI1DpYjUCbb1n0XqhblaNQFvf07ReqJtVI9CrtuibWi/Uae9/A/pWfqS6GoHuPtnROG2f7MqZBq9GoEmeysF5LZboQoc/Xzq8wVQKdHflrvvC0+c3VSnQJKuLG10gNuRURb1Ak6ySycn70X0ysc5vrWqgSXbJYzJJXhrbnNvkJZkkj17Zv7s+Htxw3s6fgari7S69q30GlU5inEFVF2936Z1nUKEZqNAMVGh3XT8+T/ob9+n++Z3l6Nb49Tmt06+Pl3ihGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrN54MKbQjPB+369d/68cvRV+v09XuJF5qBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0Hw+qNB8PugZPTy/s+vjl6Ov1unr9xIvNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1Ch+XxQofl80DN6eH5n18cvR1+t09fvJV5oBio0AxWagQrtewQ6TtbJJnlL3pJNsk7G5WcJ6L4cDMwhzdHH4SRJMk32yVOy+/hRkQz6DDpPtv+rs2mUbJN5ORbHcAOdJ8ty9mdLG+UaaKDji+s8WHpLCjXQQNfl4LwWS9S9IQY6Pnnf+ZmRJ1GiIQa6KAeXar1QnRlioA/l4FKtF6ozQwz0sM3ZQuuF6swQA9WADDHQbTm4VOuF6swQA30tB5dqvVCdGWKgV23RN7VeqM4MMdBdsi9n5+39qxGiIQaa5KkcnNdiibo30EB3V+66Lzx9Qg000CSrixtdJKtyJojhBppklUxO3o/uk4l1og060CS75DGZJC+Nbc5t8pJMkkev7HRD/y8fBzt/BrpVQz+D6sYZqNAMVGgGKjQDFdq/ORrI24OGp80AAAAASUVORK5CYII=", "type": "image" }, { "name": "3", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFWUlEQVR4Xu3c71HiahxH8a93LEOsA7YPrIAG4Nax0IAVYB9AHWIf3hcM3vjs8i8h5BjOZ3zh/PTJMMyZJD7L5mEymUTCajtQj3+cxz/un3IgkRio0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEK7aHtx+dJTXgGFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqtMdy0ILX19dydD2TycTjH3GD45ejq/IMKjRSoE/J7+Rz//We/E6eyt/SXcEEOk5WyawyGSSzZGWjd40R6O7cOSjHSTJIVuVM94MR6PRAnTuDZFrOdE3DZJms9jdXq2SZDMvf6gQj0OqVfZ48JA/JW2U4rnyvKxom78k6GSej/XCUjJN18t59poxAq/7df1Ot9uu90xVNk/Wpa9e648sXL1DdxjSZl7O/m3fZKC/QZZLk6fvbt658r+aGZ9e5M+/sWs8ItPpmjZPPZPv9vrN6P6rmdmeBi9RYcg2MQBfJtpz9b5ssypnqGx697zxk0M1JlBHoR/LrQKPb5Fc5UyPVvz4vUnthA4xAs2+0uDGaJc/Jx/ehGqr9L3O1FzaACTTJR2WPaccrextq79nVXtgAKVDpDwZ6f2rv2dVe2ICB3p/a9/S1FzbACPTrM6Cf5U90fRdt0VfVXtgAI1Dd0ubAjt5x22RTzm6AEeju40u7L93ASzk4rcaSa2AEqhvbXLjrPuvm9BkDvV+LsxuddbkhbaB3bJGMjt6PbpNRl3XGQO/dJnlORslbZZtznbwlo+S5syv7l1s8uOE0d5e6tensb6CTPIMKjXEGdXdJB3gGFZqBCs1AhfbQ9uPzpCYe0/7zO8vRT+P7c1yr74+XeKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs3ngwqtD88Hbfv1//Tjl6Nra/X1e4kXmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQfD6o0Hw+6Ak3eH5n28cvR9fW6uv3Ei80AxWagQrNQIVmoEIzUKEZqNAMVGgGKjQDFZqBCs1AhWagQjNQoRmo0AxUaAYqNAMVmoEKzUCFZqBCM1ChGajQDFRoBio0AxWagQrNQIVmoEIzUKH5fFCh+XzQE27w/M62j1+Orq3V1+8lXmgGKjQDFZqBCu0+Ah0my2SVfCafySpZJsPytwT0WA56Zpfm4PtwlCQZJ9vkJdl8/6lIen0GnSbrP+qsGiTrZFqOxdHfQKfJvJz93dxGuXoa6PDsOnfm3pJC9TTQZTk4rcYSta+PgQ6P3nceMvAkStTHQGfl4Fy1F6o1fQz0qRycq/ZCtaaPge62OWuovVCt6WOg6pE+BrouB+eqvVCt6WOgH+XgXLUXqjV9DPSiLfqq2gvVmj4Gukm25ey0rZ8aIepjoEleysFpNZaofT0NdHPhrvvM0ydUTwNNsji70VmyKGeC6G+gSRbJ6Oj96DYZWSdarwNNskmek1HyVtnmXCdvySh59spO1/f/8rGz8W+gn6rvZ1D9cAYqNAMVmoEKzUCF9h+QhMjbNytVWQAAAABJRU5ErkJggg==", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "0" }, { "data": [{ "name": "4", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEeUlEQVR4Xu3c3U0cZxiA0ZfIZRjXAe6DVEADJnUYGnAFdh9AHcZ9OBcIZz0Ji9kf9nE4R1ysXpjRavVoZvhA39H5+flA1r4Ddf71nH+9P5YDKBEoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZJ2tO/t82Ar+w7U+ddz/vXc4kkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSZn9Q0lxBSRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIEStqb5WAPPn36tBztzvn5ufOv8QLnX452yhWUtFKgb2c+znx/+Po683Hm7fKneFUygZ7NXM9crEyOZy5mrjX6qjUCvb92Hi/HMzPHM9fLGa9HI9APj9R573jmw3LGLp3MfJ65fni4up75PHOy/KmDaAS6eme/nDmaOZr5sjI8W3nNDp3MfJ25mTmbOX0Yns6czdzMfD18po1AV/318GK12h+fHTv0YebmqXvXzYFvX71AeRkfZi6Xs/92echGe4F+npmZtz9/fDcrr9neyS/Xee/yYPf6RqCrH9bZzPeZu5+fO1efR9ne/VXgWTY4ZBcagV7N3C1n/7ibuVrO2NzJ2ufOxxwf5iLaCPTbzPtHGr2beb+csZXV3z6fZeMDt9AIdB4aXTwYXcy8m/n285AtbfyXuY0P3EIm0Jn5trLGdM+dfR82XrPb+MAtlAKFfxHo67Pxmt3GB25BoK/Pxs/0Gx+4hUagP/4H9PvyO+zes5boV2184BYagfKSbh9Z0VvvbuZ2OXsBjUDv/33p/osX8Ody8LQNDtmFRqC8sNtnrrpfHObyOQJ9va5+udGLQy5IC/QVu5o5Xfs8ejdzesg6R6Cv3e3Mu5nTmS8ry5w3M19mTmfeHezO/sNLbNzwNKtLh3V7sN+BnuQKSlrjCmp1iUe4gpImUNIEStrRvrfPg228mf3v37kc/W58Puvt9fNxiydNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmv1BSfs/7A+67/f/u59/Odq1vb5/t3jSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpNkflDT7gz7hBfbv3Pf5l6Nd2+v7d4snTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIEStrfVZOaF1ZBIAYAAAAASUVORK5CYII=", "type": "image" }, { "name": "5", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEfUlEQVR4Xu3c4TEkWxzG4detDQNxsHm4EUiAG8eSgAjIA3EgD/fDFHd0XYNZ3ee1nqd8mPqv7pqyP93tUGfn+Pg4UGvuQJ1/M+ff7K/pAJoIlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUajtzb58Hv2XuQJ1/M+ffzC2eagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWa/UGp5gpKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNV+TAczuLi4mI4+z/HxsfNvsMD5p6NP5QpKNYHyZDf5lTw+fdwlv5Ld6WctTKAkSY6S6+R0bbKXnCbXgxsVKE/Xzr3pOEn2kuvpbEkdgR4kl8n1083lOrlMDqafxVxOXqlzZS85mc4WMzrQg+QuuUmOksOn4WFylNwkdzJdxPqd/SzZSXaSq7Xh0drrZQ0N9CS5eet792bkt+939M/Ti/Vqn68dixsX6ElyNp39vzONfl+DAj14d50rZ+71S7lMkuy+/A+6WXu9rEGBrr4KH7LFIbzTeotHyWNy//K5c/15dFkjAj3Y+Nz5mj0X0dmcJ/fT2X/uk/PpbDEjAl1/+v6QrQ9ks4fk5yuN3ic/p7MljQh0699MbH0gb1o1OvnB4DTZTx5eDpc1ItCt1yy2PpD3eFhbY1oZd2d/NiJQeLcRgW69ZrH1gXxZIwLd+plm6wP5skYE+qEl+nVbH8hmz38D+jj9l+FGBHr7yorGZvfJ7XTGH29EoEn+ng7etsUhvNPqz5dWH2UGBXr7wVX3U5fPb2pQoEnO393oacWCHEOMCzTJeXK48Xn0PjlU57c2NNAkt8l+cphcrS1z3iRXyWGy787+3S2xccPbbv0MNFTf6tKz0VdQ2KjjCspYfatLz1xBqSZQqgmUajtzb58Hv+NH5t+/czr6anx9Npv16+MWTzWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSzf6gVPsT9ged+/1/9fNPR59t1vfvFk81gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUs3+oFSzP+gbFti/c+7zT0efbdb37xZPNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVLtX/4amhfJiDMyAAAAAElFTkSuQmCC", "type": "image" }, { "name": "6", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEfUlEQVR4Xu3c4TEkWxzG4detDQNxsHm4EUiAG8eSgAjIA3EgD/fDFHd0XYNZ3ee1nqd8mPqv7pqyP93tUGfn+Pg4UGvuQJ1/M+ff7K/pAJoIlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUajtzb58Hv2XuQJ1/M+ffzC2eagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWa/UGp5gpKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNV+TAczuLi4mI4+z/HxsfNvsMD5p6NP5QpKNYHyZDf5lTw+fdwlv5Ld6WctTKAkSY6S6+R0bbKXnCbXgxsVKE/Xzr3pOEn2kuvpbEkdgR4kl8n1083lOrlMDqafxVxOXqlzZS85mc4WMzrQg+QuuUmOksOn4WFylNwkdzJdxPqd/SzZSXaSq7Xh0drrZQ0N9CS5eet792bkt+939M/Ti/Vqn68dixsX6ElyNp39vzONfl+DAj14d50rZ+71S7lMkuy+/A+6WXu9rEGBrr4KH7LFIbzTeotHyWNy//K5c/15dFkjAj3Y+Nz5mj0X0dmcJ/fT2X/uk/PpbDEjAl1/+v6QrQ9ks4fk5yuN3ic/p7MljQh0699MbH0gb1o1OvnB4DTZTx5eDpc1ItCt1yy2PpD3eFhbY1oZd2d/NiJQeLcRgW69ZrH1gXxZIwLd+plm6wP5skYE+qEl+nVbH8hmz38D+jj9l+FGBHr7yorGZvfJ7XTGH29EoEn+ng7etsUhvNPqz5dWH2UGBXr7wVX3U5fPb2pQoEnO393oacWCHEOMCzTJeXK48Xn0PjlU57c2NNAkt8l+cphcrS1z3iRXyWGy787+3S2xccPbbv0MNFTf6tKz0VdQ2KjjCspYfatLz1xBqSZQqgmUajtzb58Hv+NH5t+/czr6anx9Npv16+MWTzWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSzf6gVPsT9ged+/1/9fNPR59t1vfvFk81gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUs3+oFSzP+gbFti/c+7zT0efbdb37xZPNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVLtX/4amhfJiDMyAAAAAElFTkSuQmCC", "type": "image" }, { "name": "7", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEeUlEQVR4Xu3c3U0cZxiA0ZfIZRjXAe6DVEADJnUYGnAFdh9AHcZ9OBcIZz0Ji9kf9nE4R1ysXpjRavVoZvhA39H5+flA1r4Ddf71nH+9P5YDKBEoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZJ2tO/t82Ar+w7U+ddz/vXc4kkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSZn9Q0lxBSRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIEStqb5WAPPn36tBztzvn5ufOv8QLnX452yhWUtFKgb2c+znx/+Po683Hm7fKneFUygZ7NXM9crEyOZy5mrjX6qjUCvb92Hi/HMzPHM9fLGa9HI9APj9R573jmw3LGLp3MfJ65fni4up75PHOy/KmDaAS6eme/nDmaOZr5sjI8W3nNDp3MfJ25mTmbOX0Yns6czdzMfD18po1AV/318GK12h+fHTv0YebmqXvXzYFvX71AeRkfZi6Xs/92echGe4F+npmZtz9/fDcrr9neyS/Xee/yYPf6RqCrH9bZzPeZu5+fO1efR9ne/VXgWTY4ZBcagV7N3C1n/7ibuVrO2NzJ2ufOxxwf5iLaCPTbzPtHGr2beb+csZXV3z6fZeMDt9AIdB4aXTwYXcy8m/n285AtbfyXuY0P3EIm0Jn5trLGdM+dfR82XrPb+MAtlAKFfxHo67Pxmt3GB25BoK/Pxs/0Gx+4hUagP/4H9PvyO+zes5boV2184BYagfKSbh9Z0VvvbuZ2OXsBjUDv/33p/osX8Ody8LQNDtmFRqC8sNtnrrpfHObyOQJ9va5+udGLQy5IC/QVu5o5Xfs8ejdzesg6R6Cv3e3Mu5nTmS8ry5w3M19mTmfeHezO/sNLbNzwNKtLh3V7sN+BnuQKSlrjCmp1iUe4gpImUNIEStrRvrfPg228mf3v37kc/W58Puvt9fNxiydNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmv1BSfs/7A+67/f/u59/Odq1vb5/t3jSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpNkflDT7gz7hBfbv3Pf5l6Nd2+v7d4snTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIEStrfVZOaF1ZBIAYAAAAASUVORK5CYII=", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "1" }, { "data": [{ "name": "8", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEcUlEQVR4Xu3c3VEUaRiA0ZctwxDjAPNgIyAB2TiEBIxA8wDiEPNwLyjcsXcZZH6Yx+Wc4mLqle6iup7qr/mk+uj8/Hwga9+BOv96zr/eH8sBlAiUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEk72vfr82Ar+w7U+ddz/vUs8aQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJ835Q2vYdqPOv5/zrWeJJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkub9oKS5g5ImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZS0N8vBHnz69Gk52p3z83PnX+MFzr8c7ZQ7KGmlQN/OfJz5/vD1debjzNvld/GqZAI9m7meuViZHM9czFxr9FVrBHp/7zxejmdmjmeulzNej0agHx6p897xzIfljF06mfk8c/3wcHU983nmZPldB9EIdHVlv5w5mjma+bIyPFv5zA6dzHyduZk5mzl9GJ7OnM3czHw9fKaNQFf99fBhtdof144d+jBz89TadXPg5asXKC/jw8zlcvbfLg/ZaC/QzzMz8/bny3ez8pntnfxynfcuD7bWNwJdvVhnM99n7n5+7lx9HmV793eBZ9ngkF1oBHo1c7ec/eNu5mo5Y3Mna587H3N8mJtoI9BvM+8fafRu5v1yxlZWf/t8lo0P3EIj0HlodPFgdDHzbubbz0O2tPH/zG184BYygc7Mt5U9pntW9n3YeM9u4wO3UAoU/kWgr8/Ge3YbH7gFgb4+Gz/Tb3zgFhqB/vgb0O/Lf2H3nrVFv2rjA7fQCJSXdPvIjt56dzO3y9kLaAR6/+dL91+8gD+Xg6dtcMguNALlhd0+c9f94jC3zxHo63X1y41eHHJDWqCv2NXM6drn0buZ00PWOQJ97W5n3s2cznxZ2ea8mfkyczrz7mAr+w8v8eKGp9ldOqzbg/0O9CR3UNIad1C7SzzCHZQ0gZImUNKO9v36PNjGm9n/+zuXo9+N67PeXq+PJZ40gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoad4PStr/4f2g+/75f/fzL0e7ttef3xJPmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlLS/AfTVlHOKn+cSAAAAAElFTkSuQmCC", "type": "image" }, { "name": "9", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEdUlEQVR4Xu3c4TEkWxzG4detDQNxsHm4EUiAG8eSgAjIA3EgD/fDFHd0XY1Z0+e1nqd8mPqv7lKzP33aoXrn+Pg4UGvbgTr/POef99d0AE0ESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKtZ1tPz4Pfsu2A3X+ec4/zxJPNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVLN80Hptu1AnX+e88+zxFNNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVPN8UKq5glJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKtR/TwRZcXFxMR5/n+PjY+WcscP7p6FO5glJNoDzZTX4lj08fd8mvZHf6WQsTKEmSo+Q6OV2b7CWnyfXgRgXK07VzbzpOkr3kejpbUkegB8llcv20uFwnl8nB9LPYlpNX6lzZS06ms8WMDvQguUtukqPk8Gl4mBwlN8mdTBexvrKfJTvJTnK1Njxae72soYGeJDdvfe/ejPz2/Y7+eXqxXu3ztWNx4wI9Sc6ms/93ptHva1CgB++uc+XMWr+UyyTJ7sv/oJu118saFOjqXfiQDQ7hndZbPEoek/uX953r96PLGhHowex952v2XES35jy5n87+c5+cT2eLGRHo+t33h2x8IPMekp+vNHqf/JzOljQi0I1/M7Hxgbxp1ejkB4PTZD95eDlc1ohAN96z2PhA3uNhbY9pZdzK/mxEoPBuIwLdeM9i4wP5skYEuvE9zcYH8mWNCPRDW/TrNj6Qec9/A/o4/ZfhRgR6+8qOxrz75HY64483ItAkf08Hb9vgEN5p9edLq48ygwK9/eCu+6nL5zc1KNAk5+9u9LRiQ44hxgWa5Dw5nL0fvU8O1fmtDQ00yW2ynxwmV2vbnDfJVXKY7FvZv7slHtzwtls/Aw3Vt7v0bPQVFGZ1XEEZq2936ZkrKNUESjWBUm1n24/Pg9/xI9t/fud09NV4f+Zt9f2xxFNNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVPN8UKr9Cc8H3fbX/9XPPx19tq1+/ZZ4qgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRq/wKda5Rzp/62cgAAAABJRU5ErkJggg==", "type": "image" }, { "name": "10", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEdUlEQVR4Xu3c4TEkWxzG4detDQNxsHm4EUiAG8eSgAjIA3EgD/fDFHd0XY1Z0+e1nqd8mPqv7lKzP33aoXrn+Pg4UGvbgTr/POef99d0AE0ESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKtZ1tPz4Pfsu2A3X+ec4/zxJPNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVLN80Hptu1AnX+e88+zxFNNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVPN8UKq5glJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKtR/TwRZcXFxMR5/n+PjY+WcscP7p6FO5glJNoDzZTX4lj08fd8mvZHf6WQsTKEmSo+Q6OV2b7CWnyfXgRgXK07VzbzpOkr3kejpbUkegB8llcv20uFwnl8nB9LPYlpNX6lzZS06ms8WMDvQguUtukqPk8Gl4mBwlN8mdTBexvrKfJTvJTnK1Njxae72soYGeJDdvfe/ejPz2/Y7+eXqxXu3ztWNx4wI9Sc6ms/93ptHva1CgB++uc+XMWr+UyyTJ7sv/oJu118saFOjqXfiQDQ7hndZbPEoek/uX953r96PLGhHowex952v2XES35jy5n87+c5+cT2eLGRHo+t33h2x8IPMekp+vNHqf/JzOljQi0I1/M7Hxgbxp1ejkB4PTZD95eDlc1ohAN96z2PhA3uNhbY9pZdzK/mxEoPBuIwLdeM9i4wP5skYEuvE9zcYH8mWNCPRDW/TrNj6Qec9/A/o4/ZfhRgR6+8qOxrz75HY64483ItAkf08Hb9vgEN5p9edLq48ygwK9/eCu+6nL5zc1KNAk5+9u9LRiQ44hxgWa5Dw5nL0fvU8O1fmtDQ00yW2ynxwmV2vbnDfJVXKY7FvZv7slHtzwtls/Aw3Vt7v0bPQVFGZ1XEEZq2936ZkrKNUESjWBUm1n24/Pg9/xI9t/fud09NV4f+Zt9f2xxFNNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVPN8UKr9Cc8H3fbX/9XPPx19tq1+/ZZ4qgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRq/wKda5Rzp/62cgAAAABJRU5ErkJggg==", "type": "image" }, { "name": "11", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEcUlEQVR4Xu3c3VEUaRiA0ZctwxDjAPNgIyAB2TiEBIxA8wDiEPNwLyjcsXcZZH6Yx+Wc4mLqle6iup7qr/mk+uj8/Hwga9+BOv96zr/eH8sBlAiUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEk72vfr82Ar+w7U+ddz/vUs8aQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJ835Q2vYdqPOv5/zrWeJJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkub9oKS5g5ImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZS0N8vBHnz69Gk52p3z83PnX+MFzr8c7ZQ7KGmlQN/OfJz5/vD1debjzNvld/GqZAI9m7meuViZHM9czFxr9FVrBHp/7zxejmdmjmeulzNej0agHx6p897xzIfljF06mfk8c/3wcHU983nmZPldB9EIdHVlv5w5mjma+bIyPFv5zA6dzHyduZk5mzl9GJ7OnM3czHw9fKaNQFf99fBhtdof144d+jBz89TadXPg5asXKC/jw8zlcvbfLg/ZaC/QzzMz8/bny3ez8pntnfxynfcuD7bWNwJdvVhnM99n7n5+7lx9HmV793eBZ9ngkF1oBHo1c7ec/eNu5mo5Y3Mna587H3N8mJtoI9BvM+8fafRu5v1yxlZWf/t8lo0P3EIj0HlodPFgdDHzbubbz0O2tPH/zG184BYygc7Mt5U9pntW9n3YeM9u4wO3UAoU/kWgr8/Ge3YbH7gFgb4+Gz/Tb3zgFhqB/vgb0O/Lf2H3nrVFv2rjA7fQCJSXdPvIjt56dzO3y9kLaAR6/+dL91+8gD+Xg6dtcMguNALlhd0+c9f94jC3zxHo63X1y41eHHJDWqCv2NXM6drn0buZ00PWOQJ97W5n3s2cznxZ2ea8mfkyczrz7mAr+w8v8eKGp9ldOqzbg/0O9CR3UNIad1C7SzzCHZQ0gZImUNKO9v36PNjGm9n/+zuXo9+N67PeXq+PJZ40gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoad4PStr/4f2g+/75f/fzL0e7ttef3xJPmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlLS/AfTVlHOKn+cSAAAAAElFTkSuQmCC", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "2" }, { "data": [{ "name": "12", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEZ0lEQVR4Xu3c3U0cSRhA0Y+VwzCOA5wHGwEJwMZhSMAR2HkAcRjn4X1AeMe9y2Dmh7krzhEPo4IuoeGqqilQH52fnw9k7TtQ869n/vX+WA5AiUBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlLSjfT8+D7ay70DNv57517PFkyZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiXN80Fp23eg5l/P/OvZ4kkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGS5vmgtO07UPOvZ/71bPGkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASfN8UNKsoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiXt3XJgDz5//rwc2p3z83Pzr/EK8y+HdsoKSlop0Pczn2Z+PH58m/k08375VbwpmUDPZm5mLldGjmcuZ240+qY1An1YO4+XwzMzxzM3yzHejkagF0/U+eB45mI5xi6dzHyZuXm8ubqZ+TJzsvyqg2gEurqzX80czRzNfF0ZPFt5zQ6dzHybuZ05mzl9HDydOZu5nfl2+Ewbga766/HFarU/3zt26GLm9rm96/bA21cvUF7HxczVcuy/XR2y0V6gX2Zm5v2vb9/tymu2d/LbdT64Othe3wh09c06m/kxc//rfefq/Sjbe1gFXmSDS3ahEej1zP1y7B/3M9fLMTZ3sva+8ynHh1lEG4F+n/n4RKP3Mx+XY2xl9bfPF9n4wi00Ap3HRhc3RpczH2a+/zrIljb+y9zGF24hE+jMfF85Y3pgZ9+Hjc/sNr5wC6VA4V8E+vZsfGa38YVbEOjbs/E9/cYXbqER6M//Af2x/Ay796Ij+lUbX7iFRqC8prsnTvTWu5+5W469gkagD/++9PDBK/hzOfC8DS7ZhUagvLK7F566Xx5m+RyBvl3Xv93o5SEPpAX6hl3PnK69H72fOT1knSPQt+5u5sPM6czXlWPO25mvM6czHw62s//0Gg9ueJ7TpcO6O9jvQM+ygpLWWEGdLvEEKyhpAiVNoKQd7fvxebCNd/P/f37ncmjXfP9r7Pvna4snTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIEStrfkrKUz3Ns5eAAAAAASUVORK5CYII=", "type": "image" }, { "name": "13", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEaklEQVR4Xu3c4TEtSQCG4c/WDQNxcPOwEUiAG8clARGQB+JAHvbHKfaYWoNznenPep7y41QzXYpX92hqdo6PjwO1th2o+eeZf95f0wFoIlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKrtbPvxefBHth2o+eeZf54tnmoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmueD0m3bgZp/nvnn2eKpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKp5Pijdth2o+eeZf54tnmoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmueDUs0KSjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVfkwHtuDi4mI69HmOj4/NP2OB+adDn8oKSjWB8mQ3+Z08Pr3dJb+T3elHLUygJEmOkuvkdG1kLzlNrgc3KlCe1s696XCS7CXX07EldQR6kFwm10+by3VymRxMP4ptOXmlzpW95GQ6tpjRgR4kd8lNcpQcPg0eJkfJTXIn00Ws7+xnyU6yk1ytDR6tvV7W0EBPkpu3fnZvRv74fke/nl6sV/u8dixuXKAnydl07L+dafT7GhTowbvrXDmz1y/lMkmy+/IbdLP2elmDAl19FT5kg0t4p/UWj5LH5P7lfef6/eiyRgR6MHvf+Zo9i+jWnCf307F/3Sfn07HFjAh0/e77Qza+kHkPyc9XGr1Pfk7HljQi0I3/MrHxhbxp1ejkF4PTZD95eDm4rBGBbnxmsfGFvMfD2hnTyrid/dmIQOHdRgS68ZnFxhfyZY0IdON7mo0v5MsaEeiHjujXbXwh857/B/Rx+p7hRgR6+8qJxrz75HY6xv/eiECT/D0deNsGl/BOq39fWr2VGRTo7QdP3U8tn9/UoECTnL+70dOKAzmGGBdokvPkcPZ+9D45VOe3NjTQJLfJfnKYXK0dc94kV8lhsm9n/+6WeHDD2279DjRU3+nSs9ErKMzqWEEZq+906ZkVlGoCpZpAqbaz7cfnwZ/4ka///M7p0Gfz+c/Y9vfXFk81gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUu0fO0iUz44D2QoAAAAASUVORK5CYII=", "type": "image" }, { "name": "14", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEaklEQVR4Xu3c4TEtSQCG4c/WDQNxcPOwEUiAG8clARGQB+JAHvbHKfaYWoNznenPep7y41QzXYpX92hqdo6PjwO1th2o+eeZf95f0wFoIlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKrtbPvxefBHth2o+eeZf54tnmoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmueD0m3bgZp/nvnn2eKpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKp5Pijdth2o+eeZf54tnmoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmueDUs0KSjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVfkwHtuDi4mI69HmOj4/NP2OB+adDn8oKSjWB8mQ3+Z08Pr3dJb+T3elHLUygJEmOkuvkdG1kLzlNrgc3KlCe1s696XCS7CXX07EldQR6kFwm10+by3VymRxMP4ptOXmlzpW95GQ6tpjRgR4kd8lNcpQcPg0eJkfJTXIn00Ws7+xnyU6yk1ytDR6tvV7W0EBPkpu3fnZvRv74fke/nl6sV/u8dixuXKAnydl07L+dafT7GhTowbvrXDmz1y/lMkmy+/IbdLP2elmDAl19FT5kg0t4p/UWj5LH5P7lfef6/eiyRgR6MHvf+Zo9i+jWnCf307F/3Sfn07HFjAh0/e77Qza+kHkPyc9XGr1Pfk7HljQi0I3/MrHxhbxp1ejkF4PTZD95eDm4rBGBbnxmsfGFvMfD2hnTyrid/dmIQOHdRgS68ZnFxhfyZY0IdON7mo0v5MsaEeiHjujXbXwh857/B/Rx+p7hRgR6+8qJxrz75HY6xv/eiECT/D0deNsGl/BOq39fWr2VGRTo7QdP3U8tn9/UoECTnL+70dOKAzmGGBdokvPkcPZ+9D45VOe3NjTQJLfJfnKYXK0dc94kV8lhsm9n/+6WeHDD2279DjRU3+nSs9ErKMzqWEEZq+906ZkVlGoCpZpAqbaz7cfnwZ/4ka///M7p0Gfz+c/Y9vfXFk81gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUu0fO0iUz44D2QoAAAAASUVORK5CYII=", "type": "image" }, { "name": "15", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEZ0lEQVR4Xu3c3U0cSRhA0Y+VwzCOA5wHGwEJwMZhSMAR2HkAcRjn4X1AeMe9y2Dmh7krzhEPo4IuoeGqqilQH52fnw9k7TtQ869n/vX+WA5AiUBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlLSjfT8+D7ay70DNv57517PFkyZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiXN80Fp23eg5l/P/OvZ4kkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGS5vmgtO07UPOvZ/71bPGkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASfN8UNKsoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiXt3XJgDz5//rwc2p3z83Pzr/EK8y+HdsoKSlop0Pczn2Z+PH58m/k08375VbwpmUDPZm5mLldGjmcuZ240+qY1An1YO4+XwzMzxzM3yzHejkagF0/U+eB45mI5xi6dzHyZuXm8ubqZ+TJzsvyqg2gEurqzX80czRzNfF0ZPFt5zQ6dzHybuZ05mzl9HDydOZu5nfl2+Ewbga766/HFarU/3zt26GLm9rm96/bA21cvUF7HxczVcuy/XR2y0V6gX2Zm5v2vb9/tymu2d/LbdT64Othe3wh09c06m/kxc//rfefq/Sjbe1gFXmSDS3ahEej1zP1y7B/3M9fLMTZ3sva+8ynHh1lEG4F+n/n4RKP3Mx+XY2xl9bfPF9n4wi00Ap3HRhc3RpczH2a+/zrIljb+y9zGF24hE+jMfF85Y3pgZ9+Hjc/sNr5wC6VA4V8E+vZsfGa38YVbEOjbs/E9/cYXbqER6M//Af2x/Ay796Ij+lUbX7iFRqC8prsnTvTWu5+5W469gkagD/++9PDBK/hzOfC8DS7ZhUagvLK7F566Xx5m+RyBvl3Xv93o5SEPpAX6hl3PnK69H72fOT1knSPQt+5u5sPM6czXlWPO25mvM6czHw62s//0Gg9ueJ7TpcO6O9jvQM+ygpLWWEGdLvEEKyhpAiVNoKQd7fvxebCNd/P/f37ncmjXfP9r7Pvna4snTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIEStrfkrKUz3Ns5eAAAAAASUVORK5CYII=", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "3" }, { "data": [{ "name": "16", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEXUlEQVR4Xu3c3U1bSRiA4Y/VlhFSB6QPtgIaCFtHoIFUkPQB1BHSR/YCkXXOLib4B7+Sn0dcWIPPXFivZo4HdE4uLy8HsvYdqPnXM/96fywHoESgpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBErayb4fnwdb2Xeg5l/P/OvZ4kkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGS5vmgtO07UPOvZ/71bPGkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASfN8UNr2Haj51zP/erZ40gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKR5Piht+w7U/OuZfz1bPGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSPB+UNCsoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASftzObAHnz9/Xg7tzuXlpfnXeIP5l0M7ZQUlrRTou5lPMz+efr7NfJp5t3wXRyUT6MXM7czVysjpzNXMrUaPWiPQx7XzdDk8M3M6c7sc43g0Av34TJ2PTmc+LsfYpbOZLzO3TzdXtzNfZs6W7zqIRqCrO/v1zMnMyczXlcGLldfs0NnMt5m7mYuZ86fB85mLmbuZb4fPtBHoqr+fXqxW+/OzY4c+zty9tHfdHXj76gXK2/g4c70c+3/Xh2y0F+iXmZl59+vHd7fymu2d/Xadj64Pttc3Al39sC5mfsw8/HrfuXo/yvYeV4FX2eCSXWgEejPzsBz718PMzXKMzZ2tve98zulhFtFGoN9nPjzT6MPMh+UYW1n99vkqG1+4hUag89To4sboaub9zPdfB9nSxn+Z2/jCLWQCnZnvK2dMj+zs+7Dxmd3GF26hFCj8h0CPz8ZndhtfuAWBHp+N7+k3vnALjUB//g/oj+Vv2L1XHdGv2vjCLTQC5S3dP3Oit97DzP1y7A00An3896XHH97AX8uBl21wyS40AuWN3b/y1P3qMMvnCPR43fx2o1eHPJAW6BG7mTlfez/6MHN+yDpHoMfufub9zPnM15VjzruZrzPnM+8PtrP/9BYPbniZ06XDuj/Yd6AXWUFJa6ygTpd4hhWUNIGSJlDS/gGVB48rdMwvRQAAAABJRU5ErkJggg==", "type": "image" }, { "name": "17", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEYUlEQVR4Xu3c0VEjRwBF0YfLYcDGAZsHjoAEYONYSIAIIA9EHIg88IcKLKbMAPJq+q05p/hQNUx/wKV71FBzcHZ2Fqi170DNP8/88/6YDkATgVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUu1g34/Pg/9k34Gaf57559niqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqeT4o3fYdqPnnmX+eLZ5qAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZrng9Jt34Gaf57559niqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqeT4o3fYdqPnnmX+eLZ5qAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZrng1LNCko1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1f6cDuzB9fX1dOjXOTs7M/+MBeafDv1SVlCqCZRnh8nP5On54yH5mRxOv2phAiVJcprcJRdbI0fJRXI3uFGB8rx2Hk2Hk+QouZuOLakj0OPkJrl73lzukpvkePpV7Mv5G3VuHCXn07HFjA70OHlIVslpcvI8eJKcJqvkQaaL2N7ZL5OD5CC53Ro83Xq9rKGBnier9353VyN/fb+iH88vtqt9WTsWNy7Q8+RyOvbvLjX6dQ0K9PjDdW5c2uuXcpMkOXz9A1ptvV7WoEA334VP2eESPmi7xdPkKVm/vu/cvh9d1ohAj2fvO99yZBHdm6tkPR37xzq5mo4tZkSg23ffn7Lzhcx7TL6/0eg6+T4dW9KIQHf+y8TOF/KuTaOTNwYXybfk8fXgskYEuvOZxc4X8hGPW2dMG+N29hcjAoUPGxHozmcWO1/Ib2tEoDvf0+x8Ib+tEYF+6oh+284XMu/lf0Cfpp8ZbkSg92+caMxbJ/fTMf73RgSa5K/pwPt2uIQP2vz70uajzKBA7z956n5h+fyiBgWa5OrDjV5UHMgxxLhAk1wlJ7P3o+vkRJ1f2tBAk9wn35KT5HbrmHOV3CYnyTc7+1e3xIMb3nfvPdBQfadLL0avoDCrYwVlrL7TpRdWUKoJlGoCpdrfPZ2PKz3yg+4AAAAASUVORK5CYII=", "type": "image" }, { "name": "18", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEYUlEQVR4Xu3c0VEjRwBF0YfLYcDGAZsHjoAEYONYSIAIIA9EHIg88IcKLKbMAPJq+q05p/hQNUx/wKV71FBzcHZ2Fqi170DNP8/88/6YDkATgVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUu1g34/Pg/9k34Gaf57559niqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqeT4o3fYdqPnnmX+eLZ5qAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZrng9Jt34Gaf57559niqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqeT4o3fYdqPnnmX+eLZ5qAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZpAqSZQqgmUagKlmkCpJlCqCZRqAqWaQKkmUKoJlGoCpZrng1LNCko1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1QRKNYFSTaBUEyjVBEo1gVJNoFQTKNUESjWBUk2gVBMo1f6cDuzB9fX1dOjXOTs7M/+MBeafDv1SVlCqCZRnh8nP5On54yH5mRxOv2phAiVJcprcJRdbI0fJRXI3uFGB8rx2Hk2Hk+QouZuOLakj0OPkJrl73lzukpvkePpV7Mv5G3VuHCXn07HFjA70OHlIVslpcvI8eJKcJqvkQaaL2N7ZL5OD5CC53Ro83Xq9rKGBnier9353VyN/fb+iH88vtqt9WTsWNy7Q8+RyOvbvLjX6dQ0K9PjDdW5c2uuXcpMkOXz9A1ptvV7WoEA334VP2eESPmi7xdPkKVm/vu/cvh9d1ohAj2fvO99yZBHdm6tkPR37xzq5mo4tZkSg23ffn7Lzhcx7TL6/0eg6+T4dW9KIQHf+y8TOF/KuTaOTNwYXybfk8fXgskYEuvOZxc4X8hGPW2dMG+N29hcjAoUPGxHozmcWO1/Ib2tEoDvf0+x8Ib+tEYF+6oh+284XMu/lf0Cfpp8ZbkSg92+caMxbJ/fTMf73RgSa5K/pwPt2uIQP2vz70uajzKBA7z956n5h+fyiBgWa5OrDjV5UHMgxxLhAk1wlJ7P3o+vkRJ1f2tBAk9wn35KT5HbrmHOV3CYnyTc7+1e3xIMb3nfvPdBQfadLL0avoDCrYwVlrL7TpRdWUKoJlGoCpdrfPZ2PKz3yg+4AAAAASUVORK5CYII=", "type": "image" }, { "name": "19", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAEXUlEQVR4Xu3c3U1bSRiA4Y/VlhFSB6QPtgIaCFtHoIFUkPQB1BHSR/YCkXXOLib4B7+Sn0dcWIPPXFivZo4HdE4uLy8HsvYdqPnXM/96fywHoESgpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBErayb4fnwdb2Xeg5l/P/OvZ4kkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGS5vmgtO07UPOvZ/71bPGkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASfN8UNr2Haj51zP/erZ40gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKR5Piht+w7U/OuZfz1bPGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSPB+UNCsoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASftzObAHnz9/Xg7tzuXlpfnXeIP5l0M7ZQUlrRTou5lPMz+efr7NfJp5t3wXRyUT6MXM7czVysjpzNXMrUaPWiPQx7XzdDk8M3M6c7sc43g0Av34TJ2PTmc+LsfYpbOZLzO3TzdXtzNfZs6W7zqIRqCrO/v1zMnMyczXlcGLldfs0NnMt5m7mYuZ86fB85mLmbuZb4fPtBHoqr+fXqxW+/OzY4c+zty9tHfdHXj76gXK2/g4c70c+3/Xh2y0F+iXmZl59+vHd7fymu2d/Xadj64Pttc3Al39sC5mfsw8/HrfuXo/yvYeV4FX2eCSXWgEejPzsBz718PMzXKMzZ2tve98zulhFtFGoN9nPjzT6MPMh+UYW1n99vkqG1+4hUag89To4sboaub9zPdfB9nSxn+Z2/jCLWQCnZnvK2dMj+zs+7Dxmd3GF26hFCj8h0CPz8ZndhtfuAWBHp+N7+k3vnALjUB//g/oj+Vv2L1XHdGv2vjCLTQC5S3dP3Oit97DzP1y7A00An3896XHH97AX8uBl21wyS40AuWN3b/y1P3qMMvnCPR43fx2o1eHPJAW6BG7mTlfez/6MHN+yDpHoMfufub9zPnM15VjzruZrzPnM+8PtrP/9BYPbniZ06XDuj/Yd6AXWUFJa6ygTpd4hhWUNIGSJlDS/gGVB48rdMwvRQAAAABJRU5ErkJggg==", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "4" }, { "data": [{ "name": "20", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADoUlEQVR4Xu3c4U1cRxhA0Y+IMiB9xH3QwTZgpw7bDVBB0gfpI9AH+YGsPF7kFSvmWdfROeLH0yc0WsFlBlZork6n00DW0YFa/zzrn/fLfgAlAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0q6Ovj4P3uXoQK1/nvXPc8STJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJc39oLQdHaj1z7P+eY540gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKS5H5S065m5v7/fj9c5nU5Hr78f8drRX/9D13fEkyZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiXN/aCkXe8H/Meh91/+gA3i6Nd/6PqOeNIEus7NzOeZ528ff898nrnZfxYXEegidzMPM582k9uZTzMPGn0Xga7wsnfe7sczM7czD/sZbyfQFT5+p84XtzMf9zPeSKArbE/2LzNXM1czf26Gd5tnLiHQ1X7/9rCt9rfNM5cQKGkCXe2PmZm5mfmyGf61eeYSAl1h2+LdzPPM4+vfO7e/j3IJga7wdeZxP/vX48zX/Yw3EugKTzMfvtPo48yH/Yy3E+giL41uz/qZ+TTz68zT6yGXEOg6T5v3mF442d9NoKQJlDSBkuY/6ld43g9YxQ5Kmh10havNs910KTsoaQIlTaCkCZQ0fySt4A+jw9hBSbODrrB9m4ml7KCkCZQ0gZLmflDSrucnvz/yB/yAef1nHP39dcSTJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJc39oKT9H+4H/dnX349WO/r1H7q+I540gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaf8AaR113/M28sgAAAAASUVORK5CYII=", "type": "image" }, { "name": "21", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADjklEQVR4Xu3c0U1bSRSA4eNVyoDtI+mDDtxA2DqSNEAFmz5IHwt9sA8o2suVYvAylv5I3ycero7QyIJfM7Yf5nA8HgeyLh2o9U+z/ml/7AdQIlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJe1w6evz4F0uHaj1T7P+aY540gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKS5H5S2Swdq/dOsf5ojnjSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhp7gcl7cPM3N3d7cfrHI/HS6+/H/HSpf/+F13fEU+aQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNPeDkuZ+0Nf97q//t+aIJ02g61zNfJl5+vnzz8yXmav9b3EWgS5yM3M/c7uZXM/cztxr9F0EusLz3nm9H8/MXM/c72e8nUBX+PyLOp9dz3zez3gjga6wPdm/zhxmDjPfN8ObzTPnEOhqf/182Fb7cfPMOQRKmkBX+3tmZq5mvm6GPzbPnEOgK2xbvJl5mnl4+b5z+36Ucwh0hW8zD/vZfx5mvu1nvJFAV3ic+fSLRh9mPu1nvJ1AF3ludHvWz8ztzJ8zjy+HnEOg6zxuvmN65mR/N4GSJlDSBErah/2A/+FpP2AVOyhpdtAVDptnu+lSdlDSBEqaQEkTKGk+JK3gg9HF2EFJs4OusP2aiaXsoKQJlDSBkuZ+UNLcD/o6r/+ES/9/HfGkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASXM/KGnuB33dpV+/9U9wxJMmUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIl7V8QnncLcM5/VwAAAABJRU5ErkJggg==", "type": "image" }, { "name": "22", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADwklEQVR4Xu3c0VEbVxSA4eMMZQTXAYXEFagBKMQ0QAW4D0MfpA/ngRdYJ0JasZ7f0fc9Hrh3NPCzK3bgftrtdgNZWwdq//3sv98fywGUCJQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASfu09fF5cJKtA7X/fvbfzy2eNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGnOB6Vt60Dtv5/993OLJ02gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqa80FJu5iZ+/v75fjj7Ha7rfdfjnhr66//pvu7xZMmUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlzfmgpF0sB/xk0/Mvf8EFYuvXv+n+bvGkCZQ0gZImUNIE+hGuZh5mvs/8mPkx833mYeZq+Vms4Lf407ykefl2eD0zM3/NPM98mXl6+1GO4Qp6gpuZx5/qfO1y5nHmZjnmcAJd62bm63L2775qdD2BrnJ1cJ0vvnpLupJAV3lYDt63YgkCXeNq7/vO/3LpIrqGQI93uxwcavXCMybQ4/25HBxq9cIzJtDjvTzmXGH1wjMmUNIEerzH5eBQqxeeMYEe7+/l4FCrF54xgR7vqEf0r61eeMYEerynmefl7H3P/mpkDYGu8mU5eN+KJQh0pacjn7rfunyuJNC17g5u9HbmbjnjQAI9wd3M9d73o88z1+o8iUBP8zTzeeZ65turx5yPM99mrmc+u7Ofyr98fIQnvwNtxRWUNIGSJlDSBEqaQElzPihpF/Obnx/5C37AvP49tv7+usWTJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJc35oKT9H84H/d33X44+2tavf9P93eJJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkvYPS4t8/G0ER2wAAAAASUVORK5CYII=", "type": "image" }, { "name": "23", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADr0lEQVR4Xu3c0VEbVxSA4eOMywiuAwqJK1ADUAg0QAW4D0MfpA/ngRfYJJJ2hWZ+j77v8cC9o4F/dsWOuF92u91A1rkDtf9+9t/vj+UASgRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKR9OffxeXCScwdq//3sv59bPGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSnA9K27kDtf9+9t/PLZ40gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoac4HJe3rzDw+Pi7Hn2e32517/+WIj8798z/r/m7xpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQElzPihpzgc97Hd//b81t3jSBEqaQEkTKGkC/QzXM08zP2d+zfya+TnzNHO9/C42+LocsMpbmlcfhzczM/PXzOvM95mXj19lDVfQE9zOPP+rzveuZp5nbpdjjifQrW5n7pez/3av0e0Eusn10XW+ufeWdCOBbvK0HBy2YQkC3eJ67/vO/3PlIrqFQNe7Ww6OtXnhBRPoen8uB8favPCCCXS9t8ecG2xeeMEESppA13teDo61eeEFE+h6fy8Hx9q88IIJdL1Vj+jf27zwggl0vZeZ1+XssFefGtlCoJt8Xw4O27AEgW70svKp+53L50YC3erh6EbvZh6WM44k0BM8zNzsfT/6OnOjzpMI9DQvM99mbmZ+vHvM+TzzY+Zm5ps7+6n8y8dnePE30Lm4gpImUNIESppASRMoac4HJc35oId5/Xuc+/frFk+aQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNOeDkuZ80MPO/frtv4dbPGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDS/gHy/X4o0tc9mgAAAABJRU5ErkJggg==", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "5" }, { "data": [{ "name": "24", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADjUlEQVR4Xu3cwU0cWRRA0dcjhwGTh50HGXQCZuKwnQAR2HngPAx54AWypijJZdpUSXdxjliUntAXoq/+7+7FP53P54GsowO1/jbrb/tnPYASgZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaaejr8+DNzk6UOtvs/42RzxpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0twPStvRgVp/m/W3OeJJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkuZ+UNqODtT626y/zRFPmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDT3g5L2bmbu7u7W4/2cz+ej11+PeOno//+h6zviSRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZLmflDS3A/6Z/7+DUe/vo540gS6n6uZTzNPv35+zHyauVr/FhcR6E5uZu5nbheT65nbmXuNvolA9/C8d16vxzMz1zP36xmvJ9A9fPxNnc+uZz6uZ7ySQPewPNk/z5xmTjPfFsObxTOXEOje/vv1sKz2/eKZSwiUNIHu7evMzFzNfF4Mvy+euYRA97Bs8Wbmaebh5fvO5ftRLiHQPXyZeVjP/vcw82U945UEuofHmQ+/afRh5sN6xusJdCfPjS7P+pm5nfl35vHlkEsIdD+Pi++YnjnZ30ygpAmUNIGS9m494C88rQfsxQ5Kmh10D6fFs910V3ZQ0gRKmkBJEyhpPiTtwQejw9hBSbOD7mH5NRO7soOSJlDSBEqa+0FJcz/on/n7Nxz9+jriSRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZL2Ex3AczuVRfR4AAAAAElFTkSuQmCC", "type": "image" }, { "name": "25", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADjUlEQVR4Xu3cwU0cWRRA0dcjhwGTh50HGXQCZuKwnQAR2HngPAx54AWypijJZdpUSXdxjliUntAXoq/+7+7FP53P54GsowO1/jbrb/tnPYASgZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaaejr8+DNzk6UOtvs/42RzxpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0twPStvRgVp/m/W3OeJJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkuZ+UNqODtT626y/zRFPmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDT3g5L2bmbu7u7W4/2cz+ej11+PeOno//+h6zviSRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZLmflDS3A/6Z/7+DUe/vo540gS6n6uZTzNPv35+zHyauVr/FhcR6E5uZu5nbheT65nbmXuNvolA9/C8d16vxzMz1zP36xmvJ9A9fPxNnc+uZz6uZ7ySQPewPNk/z5xmTjPfFsObxTOXEOje/vv1sKz2/eKZSwiUNIHu7evMzFzNfF4Mvy+euYRA97Bs8Wbmaebh5fvO5ftRLiHQPXyZeVjP/vcw82U945UEuofHmQ+/afRh5sN6xusJdCfPjS7P+pm5nfl35vHlkEsIdD+Pi++YnjnZ30ygpAmUNIGS9m494C88rQfsxQ5Kmh10D6fFs910V3ZQ0gRKmkBJEyhpPiTtwQejw9hBSbOD7mH5NRO7soOSJlDSBEqa+0FJcz/on/n7Nxz9+jriSRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZL2Ex3AczuVRfR4AAAAAElFTkSuQmCC", "type": "image" }, { "name": "26", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADrklEQVR4Xu3c0U0cVxSA4ePIZQTXAYXEFWwDUIhpgApwH4Y+SB/OAy8wSYadWVb6pf2+xwP3asX+mllGcL8cDoeBrHMHav919l/3x3IAJQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNK+nPv4PDjJuQO1/zr7r3OLJ02gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqa80FpO3eg9l9n/3Vu8aQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJcz4obecO1P7r7L/OLZ40gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoac4HJe3rzDw8PCzHn+dwOJx7/+WI98798z/r/m7xpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQElzPihpzgf9mNe/4tzvr1s8aQIlTaCkCZQ0gX6G65nHmV8zv2d+z/yaeZy5Xn4XO3xdDtjkNc2r98ObmZn5a+Zl5vvM8/uvsoUr6AluZ57+VedbVzNPM7fLMccT6F63Mz+Ws//2Q6P7CXSX66PrfPXDR9KdBLrL43LwsR1LEOge16ufO//PlYvoHgLd7m45ONbuhRdMoNv9uRwca/fCCybQ7V4fc+6we+EFEyhpAt3uaTk41u6FF0yg2/29HBxr98ILJtDtNj2if2v3wgsm0O2eZ16Ws4+9+KuRPQS6y/fl4GM7liDQnZ43PnW/c/ncSaB73R/d6N3M/XLGkQR6gvuZm9XPoy8zN+o8iUBP8zzzbeZm5uebx5xPMz9nbma+ubOfyr98fIZnvwOdiysoaQIlTaCkCZQ0gZLmfFDSnA/6Ma9/xbnfX7d40gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKT9A3V1eliqPDQMAAAAAElFTkSuQmCC", "type": "image" }, { "name": "27", "source": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAADrklEQVR4Xu3c0U0cVxSA4ePIZQTXAYXEFWwDUIhpgApwH4Y+SB/OAy8wSYadWVb6pf2+xwP3asX+mllGcL8cDoeBrHMHav919l/3x3IAJQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNK+nPv4PDjJuQO1/zr7r3OLJ02gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqa80FpO3eg9l9n/3Vu8aQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJcz4obecO1P7r7L/OLZ40gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoaQIlTaCkCZQ0gZImUNIESppASRMoac4HJe3rzDw8PCzHn+dwOJx7/+WI98798z/r/m7xpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQEkTKGkCJU2gpAmUNIGSJlDSBEqaQElzPihpzgf9mNe/4tzvr1s8aQIlTaCkCZQ0gX6G65nHmV8zv2d+z/yaeZy5Xn4XO3xdDtjkNc2r98ObmZn5a+Zl5vvM8/uvsoUr6AluZ57+VedbVzNPM7fLMccT6F63Mz+Ws//2Q6P7CXSX66PrfPXDR9KdBLrL43LwsR1LEOge16ufO//PlYvoHgLd7m45ONbuhRdMoNv9uRwca/fCCybQ7V4fc+6we+EFEyhpAt3uaTk41u6FF0yg2/29HBxr98ILJtDtNj2if2v3wgsm0O2eZ16Ws4+9+KuRPQS6y/fl4GM7liDQnZ43PnW/c/ncSaB73R/d6N3M/XLGkQR6gvuZm9XPoy8zN+o8iUBP8zzzbeZm5uebx5xPMz9nbma+ubOfyr98fIZnvwOdiysoaQIlTaCkCZQ0gZLmfFDSnA/6Ma9/xbnfX7d40gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKQJlDSBkiZQ0gRKmkBJEyhpAiVNoKT9A3V1eliqPDQMAAAAAElFTkSuQmCC", "type": "image" }], "layout": { "margin": { "t": 60 } }, "name": "6" }]);
          }).then(function () {
            Plotly.animate('46a64ae5-2c2c-4a65-ab24-79ed88497949', null);
          })
        };                            </script>
      </div>


      <figcaption>
        <p><strong>Figure 1: MiniGrid Memory Task Partial Observations.</strong> Above: All 4 Variations of the MiniGrid
          Memory Task as seen from the starting position. Below: A recording of high performing trajectories.</p>
      </figcaption>
    </figure>

    <p>This task is interesting for 3 reasons:</p>

    <ol>
      <li>
        <strong>The optimal policy is well described as learning a simple underlying algorithm described by the boolean
          expression A XOR B.</strong> The optimal trajectory shown in figure 1 involves walking forward 4 times, and
        turning left or right, followed by forward. However, labelling the instruction and target as boolean variables,
        the optimal policy is turn left if A XOR B and right otherwise. The XOR operation is particularly nice for
        interpretability since it is symmetric in A and B, and changing A or B will always change the correct decision.
        Therefore, all beliefs about the instruction/targets should be action guiding.
      </li>
      <li>
        <strong>Observations generated in this task include redundant, correlated and anti-correlated features,
          encouraging the use of abstractions.</strong> The gridworld environment makes this true in many ways:
        <ul>
          <li>The target configuration is detectable via the left or right position alone or via any one observation in
            a trajectory.</li>
          <li>The presence of a key at a position implies the absence of a ball at the same position (<em>hence
              instructions/targets becoming binary variables</em>).</li>
          <li>Since the instruction does not change mid-episode, observations of the same object are redundant between
            observations.</li>
        </ul>
      </li>
      <li>
        <strong>A partially observable environment forces use of the transformer’s context window.</strong> The optimal
        trajectory involves only seeing the instruction once, forcing use of the context window. This is important since
        it adds complexity which justifies the use of a transformer, which we are interested in studying.
      </li>
    </ol>

    <p><strong>Figure 2</strong> shows how the decision transformer architecture interacts with the gridworld
      observations and the central decision. We discuss tokenization of the observation in the next section.</p>

    <img src="images/image3.png" alt="Decision Transformer Diagram with Gridworld Observations">
    <strong>Figure 2:</strong> Decision Transformer Diagram with Gridworld Observations. R corresponds to the tokenized
    Reward-to-Go, S stands in for state (replaced with O in practice we have partial observations). A corresponds to
    actions tokens.

    <h3 id="#Observation-Embeddings">MemoryDT Observation Embeddings are constructed via a Compositional Code.</h3>

    <p>
      In order to adapt the Decision Transformer architecture to gridworld tasks we tokenize the observations using a <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/superposition-composition/index.html%23distributed-compositional&sa=D&source=editors&ust=1697689300201170&usg=AOvVaw18ozniSQ7pL5wvRmEeDG5Y">compositional
        code</a> whose components are “objects at (x,y)” or colour at (x,y). For example, Key at (2,3) will have its own
      embedding and so will Green (2,3) etc. <strong>Figure 3</strong> shows example observations with important
      vocabulary items shown.
    </p>

    <figure>
      <img src="images//image4.png" alt="Example Observations with annotated target/instruction vocabulary items">
      <figcaption>
        <strong>Figure 3:</strong> Example Observations with annotated target/instruction vocabulary items.
      </figcaption>
    </figure>

    <p>
      For each present vocabulary item, we learn an embedding vector. The token is then the sum of the embeddings for
      any present vocabulary items:
    </p>

    <div class="mathjax-content">
      \[
      o_t = \sum_i \sum_j \sum_c I(i,j,c) f_{i,j,c}
      \]
    </div>

    <p>
      Where \( o_t \) is the observation embedding (which is a vector of length 256), \( i \) is the horizontal
      position, \( j \) is the vertical position, \( c \) is the channel (colour, object or state), and \( f_{i,j,c} \)
      is the corresponding learned token embedding with the same dimension as the observation embedding. \( I(i,j,c) \)
      is an indicator function. For example \( I(2,6, \text{key}) \) means that there is a key at position (2,6).
    </p>

    <p><strong>Figure 4</strong> Illustrates how the observation tokens are made of embeddings, which might themselves
      be made of features, which match the task relevant concepts.</p>
    <img src="images/image5.png" alt="Illustration of how the observation tokens are made of embeddings">
    <p><strong>Figure 4: Diagram showing how concepts, features, vocabulary item embeddings and token embeddings are
        related.</strong> We learn embeddings for each vocabulary item but the model can treat those independently or
      use them to represent other features if desired. </p>

    <p><strong>A few notes on this setup:</strong></p>

    <ol>
      <li>
        Our observation tokenization method is intentionally linear and is therefore decomposable into embeddings (which
        are linear feature representations). By constructing it like this we make it harder for the model to memorise
        the observations since it must create them from a linear sum of fundamentally (more) <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/toy-double-descent/index.html%23datapoints-vs-features&sa=D&source=editors&ust=1697689300232966&usg=AOvVaw3EuWVQBopYVZi1V3sYOMX3">generalising
          features</a>. Furthermore, the function A XOR B can’t be solved using a linear classifier stopping the model
        from solving the entire task in the first observation.
      </li>
      <li>
        Our observation tokenization method is compositional with respect to vocabulary items but not task relevant
        concepts. The underlying “instruction feature” isn’t itself a member of the vocabulary.
      </li>
      <li>
        The task-relevant concepts have a many-to-many relationship with vocabulary items. There are different positions
        from which the instruction/targets might be seen.
      </li>
      <li>
        Some vocabulary items are much more important for predicting the optimal action than others. Keys/Balls are
        clearly more important, and especially so at positions from which the instruction/targets are visible.
      </li>
      <li>
        Vocabulary item embeddings will have lots of correlation structure due to partial observability of the
        environment.
      </li>
    </ol>

    <h2>Results</h2>
    <h3>Geometric Structure in Embedding Space</h3>

    <p>
      In order to determine whether MemoryDT has learned to represent the underlying task-relevant concepts, we start by
      looking at the observation embedding space.
    </p>

    <p>
      Many embeddings have much larger L2 norms than others.
    </p>

    <p>
      <b>Channels likely to be activated and likely to be important to the task such as keys/balls appeared to have the
        largest norms</b>, along with “green” and other channels which may encode useful information. Some of the
      largest embedding vectors corresponded to vocabulary items that were understandable important such as Ball (2,6),
      the ball as seen from the starting position, whilst others were less obvious Ball (0,6) which shouldn’t appear
      unless the agent moves the ball (it can do that). Embedding vectors are initialised with L2 norms of approximately
      0.32, but these vectors weren’t subject to weight decay and clearly some grew during training.
    </p>

    <img src="images/image6.png" alt="Strip Plot of L2 Norms of embedding vectors in MemoryDT’s Observation Space.">
    <strong>Figure 5:</strong> Strip Plot of L2 Norms of embedding vectors in MemoryDT’s Observation Space.

    <h3>Cosine Similarity Heatmaps reveal Geometric Structure</h3>

    <p>We initially attempted PCA / U-Map for dimensionality reduction, however neither were particularly informative.
      However, we were able to borrow the concept of a <a
        href="https://www.google.com/url?q=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5634325/&sa=D&source=editors&ust=1697689300234936&usg=AOvVaw0Q3y7DxJEiVe_o7z0ibZm-">clustergram</a>
      from systems biology. The idea is to plot a heatmap of the adjacency matrix, in this case the cosine similarity
      matrix of the embeddings and reorder rows according to a clustering algorithm. The resulting cosine similarity
      heatmaps were interesting with and without reordering of rows for clustering (<strong>Figure 6</strong>).</p>

    <img src="images/image7.png" alt="Cosine Similarity Heatmaps" />
    <strong>Figure 6:</strong> Cosine Similarity Heatmap of Embeddings for Key/Ball Channels. LHS: Order of rows/columns
    is determined by descending order given channel, y-position, x-position. The first row is Key (0,0), the next is Key
    (0,1) and so forth. RHS: Order of rows/columns is determined by agglomerative clustering. <strong>Figure 6</strong>
    is best understood via interactions (zooming/panning).

    <p>There were a number of possible stories which might explain the structural features observed in <strong>Figure
        6</strong>. Many embeddings clearly don’t have very high cosine similarity with any others. These were the
      embeddings with low norms and weren’t updated much or at all during training. There were two effects which may be
      interpreted with respect to correlation or anti-correlation:</p>

    <p>There were a number of possible stories which might explain the structural features observed in <strong>Figure
        6</strong>. Many embeddings clearly don’t have very high cosine similarity with any others. These were the
      embeddings with low norms and weren’t updated much or at all during training. There were two effects which may be
      interpreted with respect to correlation or anti-correlation:</p>

    <ol>
      <li><strong>Spatial Exclusivity/Anti-correlation</strong> was associated with <strong>antipodality</strong>:
        Without reordering, we can see off-center lines of negative cosine similarity which correspond to keys/balls at
        the same positions. This may suggest that the mutual exclusivity of keys/balls at the same position induced
        anti-correlation which led to antipodality in these representations.</li>

      <li><strong>Correlated Vocabulary</strong> items had higher cosine similarity: Some vocabulary items have
        particularly high cosine similarity with each other. For example, vocabulary items associated with one variation
        of the target configuration as seen from the starting position: key (1,2) and ball (5,2).</li>
    </ol>

    <p>To address these ideas more directly, we plotted cosine similarity as a function of whether the two vocabulary
      items shared the same channel (key or ball) or position (<strong>Figure 7</strong>).</p>

    <img src="images//image8.png" alt="Cosine Similarity Distribution" />
    <strong>Figure 7:</strong> Distribution of Cosine Similarity of pairs of embeddings/vocabulary items (limited to
    Key/Ball channels), filtered to have L2 norm above 0.8.

    <p>Even though channel/position are not a perfect proxy for correlation beyond the anti-correlation induced by
      spatial exclusivity, <strong>Figure 7</strong> shows some general trends better than <strong>Figure 6</strong>.
      Beyond potentially interesting trends (which aren’t trivial to interpret), we can see many outliers who embedding
      directions relative to each other can’t easily be interpreted without reference to the training distribution.</p>

    <p>This leads us to the hypothesis that <strong>semantic similarity</strong> may also be affecting <strong>geometric
        structure</strong>. By “semantic similarity”, we mean that maybe some vocabulary items may be related not just
      by when they are likely to occur, but by the actions which the decision transformer should make having observed
      them. To provide evidence for such a hypothesis, we focus on groups of vocabulary items with particularly absolute
      cosine similarity and clusters. For example, we observed clusters corresponding to vocabulary items in a single
      channel at multiple positions such as Keys at (0,5), (2,6) and (4,2). Interpreting these clusters was possible
      with reference to the training distribution, specifically, look at which positions the agent might be in when
      those channels activated (Figure 8).</p>

    <img src="images//image9.png" alt="Observations" />
    <strong>Figure 8:</strong> Reference Observations to assist interpretation of Feature Maps. Agent is always in
    position (3,6).

    <p>By combining the clusters observed in Figure 8 with the distribution of possible observations in the training
      dataset, it's possible to see several semantically interpretable groups:</p>
    <ol>
      <li>Targets seen from the end-of-corridor and the “look-back” position. These included Keys and Balls at (1,6) and
        (5,6).</li>
      <li>Targets as seen from the start. These included Keys and Balls at (1,2) and (5,2).</li>
      <li>Instructions as seen from various positions: These include: Start -> (2,6), Look-Back -> (4,2) (4,3), Early
        Turn 1, 2 -> (1,5), (0,5).</li>
    </ol>

    <p>At this point, we hypothesised that each of these groups of vocabulary items may contain underlying linear
      features corresponding to the semantic interpretation of the group.</p>

    <h3>Extracting and Interpreting Feature Directions in MemoryDT’s Observation Embeddings</h3>

    <p>To extract each feature direction, we perform feature extraction via Principal Component Analysis on the subset
      of relevant embedding vectors. By using PCA we hope to throw away unimportant directions while quantifying the
      variance explained by the first few directions. We can attempt to interpret both the resulting geometry of the PCA
      and the principal component directions themselves.</p>

    <p>To interpret the principal component directions, we show heatmaps of the dot product between the PC and each
      embedding vector, arranging these values to match the corresponding positions in the visualisations of gridworld
      partial observations. These heatmaps, which I call “feature maps”, have much in common with heatmaps of
      convolutional layers in vision models and represent virtual weights between each embedding the underlying
      principal component.</p>

    <h3>The Primary Instruction Feature</h3>
    <img src="images//image10.png" alt="Heatmap illustrating the Primary Instruction Feature" />
    <p><strong>Figure 9: Exploratory Analysis of the “Instruction” subset of Observation Embeddings. Left) Cosine
        Similarity Heatmap of the Instruction Embeddings. Right) 2D Scatter Plot of the first 2 Principal Components of
        a PCA generated from the embedding subset.</strong></p>

    <p>Previously, we identified that keys/balls at positions (4,2), (4,3), (0,5) and (2,6) as clustering and
      hypothesised that this may be due to an underlying “instruction feature”. The first 2 principal components of the
      PCA explain 85.12% of the variance in those embeddings and the first two dimensions create a space in which
      keys/balls appear in antipodal pairs (Figure 9). This projection is reminiscent of <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html%23phenomenology-feature-splitting&sa=D&source=editors&ust=1697689300239283&usg=AOvVaw3DqSA3BrXBCqATTzW_jVDD">feature
        splitting/anisotropic superposition</a> which is thought to occur when highly correlated features have similar
      output actions and <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html%23geometry&sa=D&source=editors&ust=1697689300239477&usg=AOvVaw1jD-x-8VPyiXyCUJC5uGPL">antipodality
        found in toy models</a>.</p>

    <p>Clearly, PC1 separates keys from balls independently of position, making it a candidate for a linear
      representation of an instruction feature. One way to interpret this is a very simple form of <a
        href="https://www.google.com/url?q=https://distill.pub/2020/circuits/equivariance/&sa=D&source=editors&ust=1697689300239861&usg=AOvVaw1dhSfxJ-SesAcq6bpO2Nvz">equivariance</a>,
      where each vocabulary item is an (in-built, not learned) feature detector akin to a neuron firing only when the
      feature is present.</p>

    <p>To visualise this instruction feature, we generate a feature map for PC1 (Figure 10), which shows that this
      feature is present to varying degrees in embeddings for keys/balls at many different positions where the
      instruction might be seen. We note that the instruction feature tends to be present at similar absolute values but
      opposite signs between keys and balls suggesting a broader symmetry in the instruction feature between keys and
      balls.</p>

    <img src="images//image11.png" alt="Feature Map showing Instruction PC1 Values" />
    <strong>Figure 10: Feature Map showing Instruction PC1 Values for all embeddings corresponding to
      Keys/Ball.</strong>

    <h2>Another Instruction Feature?</h2>

    <p>PC2 in the Instruction subset PCA is less easy to interpret. Figure 9 appears to distinguish whether the has been
      identified from “look-back” and “starting” positions. However, it appears to “flip” the effect it has for
      embeddings which correspond to “instruction is key” vs “instruction is ball”. Moreover, the feature map for PC2
      (Figure 11) shows keys and balls at (3,4) as having noticeable cosine similarity with this direction which doesn’t
      fit that interpretation. Nor does this explanation predict that keys/balls at (4,3), a position similar to the
      look-back feature barely projects onto PC2.</p>

    <p>One explanation might be that because PCA finds orthogonal directions, it fails at finding a second interpretable
      feature direction.</p>

    <img src="images//image12.png" alt="Feature Map showing Instruction PC2 Values" />
    <strong>Figure 11: Feature Map showing Instruction PC2 Values for all embeddings corresponding to
      Keys/Ball.</strong>

    <h3>Target Features</h3>

    <img src="images//image13.png" alt="Target Features Visualization" />
    <p><strong>Figure 12: Exploratory Analysis of the Target Embeddings.Left: Cosine Similarity Heatmap of the Target
        Embeddings. Right: 2D Scatter Plot of the first 2 Principal Components.</strong></p>

    <p>For the target feature, we identified two separate clusters each made up of two sets of almost antipodal pairs
      (Figure 12). The geometry here is much closer to isotropic <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html&sa=D&source=editors&ust=1697689300241776&usg=AOvVaw30mGCQr7qOBQ4keXmjiZ0D">superposition
        / toy model results</a>. The faint-checkerboard pattern suggests the slightest hint of a more general target
      feature which we suspect may be learnt if we trained MemoryDT for long enough.</p>

    <p>The first two principal components of the resulting PCA explain 83.69% of the variance in those embeddings and
      produced interpretable feature maps (Figure 13):</p>

    <ol>
      <li><strong>Starting Target Feature:</strong> PC1 can be interpreted as reflecting the configuration of the
        targets as seen from the starting position \( (1,2) \) and \( (5,2) \). There's slight evidence that targets
        seen at intermediate positions while walking up to the targets \( (1,3) \) and \( (1,4) \).</li>
      <li><strong>End Target Feature:</strong> PC2 can be interpreted as reflecting the configuration of the targets as
        seen from the end of the corridor position \( (1,2) \) and \( (5,2) \).</li>
    </ol>

    <img src="images//image14.png" alt="Cosine Similarity Heatmap of the PC1 Values" />
    <img src="images//image15.png" alt="Cosine Similarity Heatmap of the PC2 Embeddings">
    <strong>Figure 13: Feature map showing Instruction PC1 Values (above) and PC2 embedding (below) for all embeddings
      corresponding to Keys/Ball. </strong>

    <h3>Using Embedding Arithmetic to Reverse Detected Features</h3>

    <h4>Embedding Arithmetic with the Instruction Feature</h4>

    <p>We previously observed that the group of vocabulary items associated with the instruction concept were separated
      cleanly into Keys and Balls by a single principal component explaining 60% of the total variance associated with 6
      vectors included. From this, we hypothesised that this principal component reflects an underlying “instruction
      feature”. To validate this interpretation, we want to show that we can leverage this prediction in non-trivial
      ways such as by generating adversarial examples.</p>

    <p>Based on the previous result, we predicted that if we added two vocabulary items matching the opposite
      instruction (ie: if the instruction is a key, seen at (2,6), we can add a ball to (0,5) and a ball to (4,2)) and
      this would induce the model to behave as if the instruction were flipped. I've drawn a diagram below to explain
      the concept (Figure 14).</p>

    <img src="images//image16.png" alt="Diagram explaining concept of instruction feature">
    <strong>Figure 14: Diagram showing the basic inspiration behind “instruction adversaries”.</strong>

    <h4>Effectiveness of Instruction Feature Adversaries</h4>
    <img src="images//image17.gif" alt="Ball, Key-Ball sequence">

    <p><strong>Figure 15: Animation showing the trajectory associated with the Instruction Reversal Experiment.</strong>
    </p>

    <p>To test adversarial features / embedding arithmetic hypothesis, we generated a set of prompts/trajectories ending
      in a position where the model’s action preference is directly determined by the observation of the instruction
      being a key/ball (<strong>Figure 15</strong>). For each of the target/instruction configurations in <strong>Figure
        15</strong>, we generate 5 different edits (<strong>Figure 14</strong>) to the first frame:</p>

    <ul>
      <li><strong>The original first frame</strong>: This is our negative control.</li>
      <li><strong>S5 with the instruction flipped</strong>: This is our positive control. Changing the instruction from
        a key to a ball or vice versa at S5 makes the model flip its left/right preference.</li>
      <li><strong>S5 complement* instruction added at (0,5)</strong>. We expect this to partially reduce the left-right
        preference but not flip it. (Unless we also removed the original instruction)</li>
      <li><strong>S5 with the complement instruction added at (2,4)</strong>. Same as the previous one.</li>
      <li><strong>S5 with the complement instruction added at (0,5) and (2,4)</strong>. Even though this frame was not
        present in the training data, we expect it to override the detection of original instruction.</li>
    </ul>

    <p>Note that due to the tokenization of the observation, we can think of adding these vocabulary items to the input
      as adding adversarial features.</p>

    <p><em>Note: I’m using the word “complement” because if the original instruction was a key, add a ball to reverse it
        and vice versa.</em></p>

    <img src="images//image18.png" alt="Adversarial Observation Token Variations">

    <p><strong>Figure 16:</strong> Adversarial Observation Token Variations. Added objects are shown red though only the
      object embedding is added.</p>

    <p><strong>Figure 17</strong> shows us the restored logit difference for each of the three test cases Complement
      (0,5), Complement (4,2) and Complement (0,5), (4,2) using the original frame as our negative control or “clean”
      input and Instruction Flipped as our “corrupt”/positive control.</p>

    <img src="images//image19.png" alt="Restored Logit Difference">

    <p><strong>Figure 17:</strong> Restored Logit Difference between left/right for instruction feature adversaries in
      scenario 1. (MemoryDT). 8 facet images correspond to each target, instruction and RTG combination. (RTG = 0.892
      corresponds to the highest possible reward which an optimal policy would receive. RTG = 0 corresponds to no
      reward, often achieved by going to the wrong target)</p>

    <p>These results are quite exciting! We were able to predict very particular adversaries in the training data that
      would cause the model to behave (almost) as if it had seen the opposite instruction and did so from the
      feature-map (an interpretability tool).</p>

    <p>Let's break the results in <strong>Figure 17</strong> down further:</p>

    <ol>
      <li>Adding two decoys <strong>isn't as effective as reverse the original instruction</strong>. We expected that
        adding two “decoy” instructions will work as well as flipping the original instruction but the best result
        attained is 0.92 and most results are around 0.80-0.90.</li>
      <li>Adding a single decoy <strong>isn't consistently additive</strong>. If the effects were linear, we would
        expect that adding each single decoy would restore ~half the logit difference. This appears to be roughly the
        case half the time. If the effect was non-linear and we needed both to achieve the result, adding each alone
        would achieve a negligible effect. This also happens in some cases.</li>
      <li>The effect of individual decoys should be symmetric in their effects under our theory but they <strong>aren’t
          always</strong>. In the case of Ball, Ball-Key at RTG 0. Adding a key at (0,5) alone achieves 0.43 of the
        logits difference of both complements but adding a key at (4,2) achieves 0.03.</li>
    </ol>

    <h3>Proving that Instruction Feature Adversaries operate only via the Instruction Feature.</h3>

    <p>Whilst the previous results are encouraging, we would like to provide stronger evidence behind the notion that
      the projection of the embedding space into instruction feature direction is causally responsible for changing the
      output logits. To show this we provide two lines of evidence:</p>

    <ol>
      <li>We show that the adversarial inputs are genuinely changing the presence of the instruction feature.</li>
      <li>We show that we can <strong>directly intervene on the instruction feature</strong> to induce the same effects
        as the adversaries or flipping the instruction.</li>
    </ol>

    <h4>The adversarial inputs are genuinely changing the presence of the instruction feature</h4>

    <p>For each of the forward passes in the experiment above, we plot the dot product of the instruction feature with
      the observation embedding against the difference between the logits for turning left and right <em>(Figure
        18)</em>. We see that:</p>

    <ol>
      <li>The dot product of the instruction feature with the observation embedding ranges from about -2 to 2 in each
        scenario.</li>
      <li>Complement (0,5), (4,2) isn’t projecting as strongly into the instruction feature direction as the Instruction
        Flipped observation. This may explain why our restored logit differences weren’t stronger before. We weren’t
        flipping the instruction feature hard enough.</li>
      <li>Whilst flipping the sign on the instruction feature flips the action preference in most cases. However, it
        fails to do so when the target configuration is “Key-Ball” and R1TG = 0.892. We think that MemoryDT mostly wants
        to predict “A XOR B” at high R1TG and its complement at low R1TG, but it doesn’t quite do this. This will be
        discussed in later posts.</li>
    </ol>

    <img src="images//image20.png" alt="Measuring the projection of the S5 observation embeddings" />
    <figcaption>Figure 18: Measuring the projection of the S5 observation embeddings into the Instruction PC0 direction
      (x-axis) and showing the logit difference between left / right (y-axis).</figcaption>

    <h4>Direct Interventions on the Instruction Feature</h4>
    <p>We directly intervene on the instruction feature on the instruction feature in each of the scenarios tested
      above, again plotting the logit difference for the final left minus right direction <em>(Figure 19)</em>.</p>

    <p><strong>This similarity in the functions mapped by the adversarial intervention (Figure 18) and the direct
        intervention are striking!</strong> They show a very similar functional mapping from the instruction feature
      sign/magnitude to the logit difference suggesting the instruction feature entirely explains our adversarial
      results.</p>

    <img src="images//image21.png" alt="Intervened Instruction PC0 direction and logit difference" />
    <p><strong>Figure 19:</strong> Intervened Instruction PC0 direction (x-axis) and showing the logit difference
      between left / right (y-axis).</p>

    <h3>Do the Instruction Feature Adversaries Transfer?</h3>
    <p>Finally, since our explanation of the instruction feature suggests that it represents a meaningful property of
      the data and that our embedding arithmetic can be interpreted as adversaries, it is reasonable to test if those
      adversaries transfer to another model trained on the same data. MemoryDT-GatedMLP is a variant of MemoryDT which
      is vulnerable to the same adversarial features <strong>(Figure 20).</strong> </p>

    <img src="images//image22.png"
      alt="Restored Logit Difference between left/right for instruction feature adversaries" />
    <p><strong>Figure 20:</strong> Restored Logit Difference between left/right for instruction feature adversaries.
      MemoryDT+GatedMLP (RTG = 0.892).</p>

    <p><strong>Figure 20</strong> suggests the following:</p>

    <ol>
      <li><strong>Reversing the instruction feature was more effective.</strong> The effect of adding two keys or two
        balls to flip the instruction was closer to the effect of flipping the original instruction and in some cases
        exceeded it.</li>
      <li><strong>Inconsistent effect sizes and asymmetric effect sizes also appeared.</strong> As with MemoryDT, single
        complements varied in the strength of their effect on the logit difference and the same case of Ball, Ball-Key
        RTG 0 showed an effect for adding a key at (0,5) was more effective than adding a key at (4,2).</li>
    </ol>

    <p>Since MemoryDT+Gated MLP is a fairly similar model to MemoryDT, it's not particularly surprising that the
      adversaries transfer.</p>

    <h2>Discussion</h2>

    <h3>Feature Representations in GridWorld Observation Embeddings</h3>

    <h4>Reasoning about MemoryDT’s Observation Embeddings</h4>

    <p>There are a number of ways to explain our results and connect them to previous work. At the broadest level, it’s
      not surprising to see structure in our embeddings. Highly structured embeddings have been <a
        href="https://www.google.com/url?q=https://browse.arxiv.org/pdf/2205.10343.pdf&sa=D&source=editors&ust=1697689300250577&usg=AOvVaw226P3JSrqXkjaXYmYheo-A">previously
        linked</a> to generalisation and grokking in toy models and the presence of composable linear features in token
      embeddings has been <a
        href="https://www.google.com/url?q=https://scholar.google.com/scholar?cluster%3D2584655260765062813%26hl%3Den%26as_sdt%3D7,39&sa=D&source=editors&ust=1697689300250770&usg=AOvVaw095Zs67ejDimMAVmiLQy9u">known</a>
      for a long time.</p>

    <p>Moreover, there’s a fairly simple story which can be told to explain many of our observations:</p>

    <ol>
      <li>Our observation embeddings correspond to features (like a ball at (0,5)) at some level of abstraction in the
        gridworld/task. A symbolic representation shortcuts the process whereby a convolutional model would first detect
        a ball at (0,5) with our chosen architecture.</li>
      <li>These embedding-features had non-trivial patterns of cosine similarity due to partial observability and
        spatial restraints as well as correlation induced by the specific task. That correlation meant that frequently
        occurring vocabulary items tended to have embedding vectors that were larger and either aligned or antipodal
        with other vectors as determined by similar effects to those studied in <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html&sa=D&source=editors&ust=1697689300251340&usg=AOvVaw1k5NSY2Pme6X3vhcrnZlzR">Anthropic’s
          Toy Models of Superposition investigation</a>.</li>
      <li>However, clearly features like ball (0,5) don’t correspond directly to the most useful underlying concepts,
        which we think are the instruction and “targets”. Thus the model eventually learned to assign directions that
        represent features like “the instruction is a key” and since these were representing binary variables, MemoryDT
        made use of antipodal representations for the observations pertaining to instruction/target vocabulary items.
      </li>
      <li>We then saw different variations on the relationship between the embeddings and the representations of higher
        level features:
        <ul>
          <li>For the instruction feature, we saw embeddings with pairs of antipodal embedding with high cosine
            similarity. PCA of these embeddings suggests underlying geometry similar to <a
              href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&sa=D&source=editors&ust=1697689300251879&usg=AOvVaw3e72tdvBDzpVV0M4nJ0wNk">anisotropic
              superposition</a>. It seems possible, but unclear whether lower order principal components were meaningful
            there, but clear that the first component is a meaningful feature that is present in many different
            embeddings.</li>
          <li>For the target features, we saw two features (possibility with some overlap) representing the targets as
            seen from different positions in close to isotropic superposition.</li>
        </ul>
      </li>
    </ol>

    <h3>WIP: Intervening in the World Model as a type of Eval Tool</h3>

    <p>Previous work with adversaries and embeddings: <a
        href="https://www.google.com/url?q=https://aclanthology.org/2021.deelio-1.1.pdf&sa=D&source=editors&ust=1697689300253468&usg=AOvVaw2ZYNuM9ylLNgQ4IeQ_26gh">Transformer
        visualization via dictionary learning: contextualized embedding as a linear superposition of transformer
        factors</a></p>

    <p>More theoretically, my results add to a body of evidence suggesting that intervening on features may be akin to
      changing what a model "believes" which would be an incredibly powerful tool for evals. This is a bit subtle but my
      interventions make the model act as if a particular fact was different. This is very different from just making
      one action more likely. To apply this, imagine you finetune your model to the point where you can't jailbreak it
      anymore but want to keep going, making it even more unlikely for the model to ever take hostile or power-seeking
      actions. One way to do this is to use latent perturbations. Intervene on features to simulate if the model had
      been jailbroken and then make it so even that wouldn't work. This could be honeypotting on steroids. Not just
      useful because you can create situations you might not know how to prompt yourself into, but might be a much more
      efficient way to explore the space of "states of mind the AI could be in" rather than "situations it could be in".
    </p>


    <p>However, many pertinent questions are still unanswered:</p>

    <ol>
      <li>To the extent that some embeddings were represented almost antipodally, why weren’t more antipodal? It could
        be the model was simply undertrained or there could be more to it.</li>
      <li>How precisely do the feature directions representing the instructions or target formation end up present in so
        many different embeddings? Did the instruction feature first form in association with more frequently observed
        vocabulary items and then undergo a <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2301.05217&sa=D&source=editors&ust=1697689300252711&usg=AOvVaw1EiiTJWepJRSLRUx6JY0Dz">phase
          change</a> in which they spread to other embeddings or was it more continuous?</li>
      <li>What are the circuits making use of each of these features? Can we understand the learned embedding directions
        better with reference to the circuits that make use of them or by <a
          href="https://www.google.com/url?q=https://www.alignmentforum.org/posts/RFtkRXHebkwxygDe2/an-interpretability-illusion-for-activation-patching-of&sa=D&source=editors&ust=1697689300253015&usg=AOvVaw3eQctiC8oQjmYNh4wAPOff">comparing
          the directions we find to optimal causal directions</a>?</li>
    </ol>

    <h2>Adversarial Inputs</h2>
    <p>To validate our understanding of the instruction feature, we used both adversarial inputs as well as direct
      intervention on the instruction feature. We were able to correctly predict which embeddings could be used to trick
      the model and show that this effect was mediated entirely via the feature we identified.</p>

    <h2>Adversaries and Interpretability</h2>
    <p>In general, our results support previous arguments that <a
        href="https://www.google.com/url?q=https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/kYNMXjg8Tmcq3vjM6%23The_studies_of_interpretability_and_adversaries_are_inseparable_&sa=D&source=editors&ust=1697689300254446&usg=AOvVaw1AGFd9wqrUZ15h_EzWVyl_">study
        of interpretability and adversaries are inseparable</a>. However, previous results connect <a
        href="https://www.google.com/url?q=https://arxiv.org/abs/1906.00945&sa=D&source=editors&ust=1697689300254577&usg=AOvVaw0D6D0SordUEvQLlblvKjBG">adversarial</a>
      <a
        href="https://www.google.com/url?q=https://arxiv.org/abs/2206.11212&sa=D&source=editors&ust=1697689300254667&usg=AOvVaw04_JtwTdG7J2OHljKFyssj">robustness</a>
      to interpretability whereas this seems not to apply here. That the instruction feature was present in the
      embeddings of so many vocabulary items was both the reason we were able to interpret it and the reason it wasn't
      adversarially robust. Rather, MemoryDT used a coherent, interpretable strategy to detect the instruction from
      lower level features that made it vulnerable to adversarial, out-of-distribution inputs. Moreover, in order to be
      robust to the adversaries we designed and still perform well on the original training distribution, we would
      predict that MemoryDT would need to implement more complicated circuitry/representations that would be less
      interpretable.
    </p>
    <p>One possible explanation is that adversarial robustness may be generally associated with crisper abstractions
      which favour interpretability but robustness to adversaries which deliberately break with simpler abstractions may
      not. This may be important to future interpretability in the gridworld context as we could study our ability to
      interpret models like MemoryDT after training them to be adversarially robust to such examples.</p>

    <h2>Adversaries and Superposition</h2>
    <p>There are <a
        href="https://www.google.com/url?q=https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/kYNMXjg8Tmcq3vjM6%23Are_adversaries_features_or_bugs_&sa=D&source=editors&ust=1697689300255226&usg=AOvVaw2TgEvrn-eyaFfJFo_WNpGJ">many
        reasons</a> to think that <a
        href="https://www.google.com/url?q=https://proceedings.neurips.cc/paper_files/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf&sa=D&source=editors&ust=1697689300255345&usg=AOvVaw1YPHtvI_NPOwRkStr8WU1a">adversaries
        are not bugs, they are features</a>. However, it has been suggested that <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html%23adversarial&sa=D&source=editors&ust=1697689300255436&usg=AOvVaw0bel1y8WAUV8eJTj9AHmnj">vulnerability
        to adversarial examples</a> may be explained by superposition. The argument suggests that unrelated features in
      superposition can be adversarially perturbed, confusing the model, which would fit into the general category of
      adversaries as bugs.</p>
    <p>However, this was suggested in the context of isotropic superposition and not <a
        href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features%23discussion-superposition&sa=D&source=editors&ust=1697689300255642&usg=AOvVaw3GhsexJeuUzEy6aXT1GYK0">anisotropic
        superposition</a>. Isotropic superposition involves feature directions which aren't representing similar
      underlying objects sharing dimensions whilst anisotropic superposition may involve features that “produce similar
      actions” (or represent related underlying features).</p>
    <p>There are 2 mechanisms through which anisotropic superposition might be related to adversaries:</p>
    <ol>
      <li>Features in anisotropic superposition are more likely to be mistaken for each other and targeted adversarial
        attacks exploit this. Humans and convolutional neural networks may both be easier to trick into thinking a photo
        of a panda is a bear and vice versa because they both look similarly. These attacks seem less inherently
        dangerous.</li>
      <li>Adversarial attacks exploit antipodality of features represented in anisotropic superposition. If features are
        present in many different embeddings, or neurons, then it's possible that combinations of events which add
        antipodal represented features can flip model behaviour with respect to a given feature. In other words, there
        may be non-trivial analogies between the mechanism by which the adversary found in this analysis operates and
        broader phenomena (though this is highly speculative).</li>
    </ol>

    <p>A priori, this second mechanism may seem complicated and underspecified. For that reason it seems unlikely as
      compared to the first, but having the example in MemoryDT provides proof of concept. For example, it could be that
      the use of many different anti-correlated, antipodal representations are exactly the kind of underlying mechanism
      behind the effectiveness of <a
        href="https://www.google.com/url?q=https://arxiv.org/pdf/2307.15043.pdf&sa=D&source=editors&ust=1697689300256096&usg=AOvVaw390rNLvxmsYWAmAQTue5Uw">initial</a>
      <a
        href="https://www.google.com/url?q=https://arxiv.org/pdf/2307.02483.pdf&sa=D&source=editors&ust=1697689300256213&usg=AOvVaw3kaLB6xc5BCl52U-_ZtXK_">affirmative</a>
      <a
        href="https://www.google.com/url?q=https://arxiv.org/abs/2306.15447&sa=D&source=editors&ust=1697689300256276&usg=AOvVaw3LovGwRHOOazVqjFx4PLUR">responses</a>
      as an adversarial prompting strategy and <a
        href="https://www.google.com/url?q=https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post&sa=D&source=editors&ust=1697689300256375&usg=AOvVaw1qcL6GRef0zfItspxKFvYq">narrative-type</a>
      strategies for jailbreaking models.
    </p>

    <p>I’m excited about the prospect that this is the case, and that the corresponding features may be found via
      techniques such as use of sparse auto-encoders, it’s possible that techniques could be devised to achieve much
      stronger robustness.</p>

    <h3>Relation to Activation Addition</h3>

    <p>A method was recently <a
        href="https://www.google.com/url?q=https://arxiv.org/pdf/2308.10248.pdf&sa=D&source=editors&ust=1697689300256663&usg=AOvVaw200JOf6mdSfqtmpg-b4ULF">proposed</a>
      to steering language model generation via steering vectors via arithmetic in activation space, though similar <a
        href="https://www.google.com/url?q=https://arxiv.org/abs/2205.05124&sa=D&source=editors&ust=1697689300256742&usg=AOvVaw3RiOlt3gK6bWBJh9UX3_WA">previous</a>
      <a
        href="https://www.google.com/url?q=https://arxiv.org/abs/2304.00740&sa=D&source=editors&ust=1697689300256800&usg=AOvVaw0vj6hlA9663_IOYS1eV7uL">methods</a>
      existed which found steering vectors via stochastic gradient descent. The use of <a
        href="https://www.google.com/url?q=https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector&sa=D&source=editors&ust=1697689300256892&usg=AOvVaw1QEXidgnBCezJ_dkWcxBxZ">counterbalanced
        steering vectors</a> is justified by the need to emphasise some property in which two prompts or tokens differ,
      which is then further emphasised via use of a scaling factor which can affect steering performance.
    </p>

    <h3>We propose that the results in this analysis may be highly relevant to the study of steering vectors in the
      following ways:</h3>

    <ol>
      <li><strong>The need for counterbalanced additions is tied to antipodality.</strong> Adding a single activation
        rather than an activation difference was less effective than adding a difference. When reversing the instruction
        feature, we found that simply adding a single complement was insufficient to reverse the logit difference as
        compared to two. In both cases, we must overcome the presence of the feature/features contained in the original
        forward pass that are antipodal with the feature representations in the steering vector. This is why flipping a
        bell to a key is approximately twice as strong as adding one new key.</li>
      <li><strong>Coefficient strength may correspond to heterogeneous feature presence.</strong> During steering, it
        was found that a coefficient was useful for creating stronger steering effects. There are two ways the MemoryDT
        results may help us understand why this is necessary. It may be that language models activations also contain
        the same features but at varying magnitudes, akin to the distribution of dot products between embedding vectors
        and the instruction direction (represented in Mathjax or as an image for Figure 10) which results in different
        degrees of projection onto the instruction feature in our adversarial prompts (represented in Mathjax or as an
        image for Figure 20).</li>
    </ol>

    <p>Whilst it's possible that common explanations or correspondences here are superficial, this seems unlikely to me
      (Joseph) and I suspect that for understanding and experimenting with steering vectors, gridworld tasks may be
      quite informative.</p>

    <h2>Conclusion and Future Work</h2>

    <p>Our primary aims moving forward with this analysis are to:</p>

    <ol>
      <li><strong>MemoryDT Circuit Analysis:</strong>
        <ul>
          <li>Show how the embeddings/features are used by circuits to generate predictions about the next action.</li>
          <li>Explain why/how MemoryDT fails to flips in action preferences when it does.</li>
          <li>Study more trajectories than in this investigation.</li>
        </ul>
      </li>

      <li><strong>Studying Reward-to-Go:</strong>
        <ul>
          <li>Provide insight as to how MemoryDT conditions on RTG, and show how this affects related circuits.</li>
        </ul>
      </li>

      <li><strong>Training Dynamics:</strong>
        <ul>
          <li>Understand the training dynamics of circuits/features in MemoryDT and similar gridworld models.</li>
          <li>We’re particularly interested in understanding whether phase changes such as those associated with
            grokking can be understood with reference to features quickly “spreading” to distinct embeddings, head
            outputs or neuron activations.</li>
        </ul>
      </li>
    </ol>

    <p>However, we're also interested in continuing to explore the following topics:</p>

    <ol start="4">
      <li><strong>Superposition in the Wild:</strong> It’s clear that superposition in language models may have a very
        different flavour to superposition in Toy Models. Do Gridworld models provide an intermediate which isn’t quite
        as messy as language models but is more diverse than toy models?</li>
      <li><strong>Adversarial Inputs:</strong> What can gridworld models tell us about the relationship between
        interpretability, generalisation and robustness?</li>
      <li><strong>Steering Vectors:</strong> Are there experiments with gridworld models that substantiate possible
        connections between our results here and previous work? We've already got preliminary evidence to explain why
        sometimes injection coefficients can be arbitrarily large? (We’ve already found preliminary examples where we
        see similar behaviour).</li>
    </ol>

    <h2>Glossary</h2>

    <ul>
      <li><strong>Adversary:</strong> An adversarial input is an input optimised (by a human or by a search process) to
        fool a model. This may involve exploiting understanding of a model’s internals, such as the adversarial inputs
        in this post.</li>

      <li><strong>Antipodal:</strong> An antipodal representation is a pair of features that are opposite to each other
        while both occupying a single direction - one positive, and one negative.</li>

      <li><strong>Decision Transformer:</strong> A Decision Transformer treats reinforcement learning as a sequence
        modelling problem, letting us train a transformer to predict what a trained RL agent would do in a given
        environment. In this post, we do this on a gridworld task to train our MemoryDT agent.</li>

      <li><strong>Embedding:</strong> An embedding is the initial representation of the input before computation or
        attention is applied. In a language model, the input is the model’s vocabulary. In MemoryDT, the input is the
        7x7x20 tensor representing the model’s observations of the gridworld space.</li>

      <li><strong>Feature:</strong> A feature is any property of the input and therefore could correspond to any of the
        following:
        <ul>
          <li>A key is present at position (0,5).</li>
          <li>The instruction is a key in the current trajectory.</li>
          <li>The correct action to take according to the optimal policy is “right”.</li>
        </ul>
      </li>

      <li><strong>Gridworld:</strong> A toy environment for simple RL tasks that involves a task to be completed on a 2D
        grid. In our case, we chose the <a
          href="https://www.google.com/url?q=https://minigrid.farama.org/environments/minigrid/MemoryEnv/&sa=D&source=editors&ust=1697689300259025&usg=AOvVaw26U1gRMIj4Be_86Qb4LDBY">Memory
          environment in Minigrid</a>.</li>

      <li><strong>Instruction:</strong> An instruction is the key or ball represented at position (2, 6) directly to the
        left of the agent in the first timestep. It tells the agent which target it should go to in order to
        successfully complete the task.</li>

      <li><strong>Linear Feature Representation:</strong> A linear feature representation is when a feature is
        represented by a direction.
        <ul>
          <li>All vocabulary items have linear feature representations in so far as they each have an embedding vector
            which corresponds to them.</li>
          <li>Features which are not vocabulary items could have linear feature representations.</li>
        </ul>
      </li>

      <li><strong>Offline RL:</strong> RL that only uses previously collected data for training. Contrasted with online
        RL, where the agent learns by interacting with the environment directly. MemoryDT is trained using offline RL,
        since it does not create trajectories itself during training.</li>

      <li><strong>Principal Component Analysis:</strong> Principal component analysis, or PCA, is a dimensionality
        reduction method that is often used to reduce the number of variables of a data set, while preserving as much
        information as possible.</li>

      <li><strong>Reward-To-Go:</strong> The reward value that MemoryDT is predicting the sequence for. High values
        (0.892) imply correct sequences, while low values (0) imply the model should play incorrectly.</li>

      <li><strong>Target:</strong> Targets are the key/ball pair that the agent can move into in order to end the
        current episode. The target should match the instruction for a successful completion.</li>

      <li><strong>Vocabulary Item:</strong> A vocabulary item is something like key (2,5) or green (2,3).
        <ul>
          <li>Each vocabulary item has a corresponding embedding vector.</li>
        </ul>
      </li>

      <h2>Gratitude</h2>

      <p>This work was supported by grants from the Long Term Future Fund, as well as the Manifund Regranting program.
        I'd also like to thank Trajan house for hosting me (Joseph). I’m thankful to Jay for his TODO: Complete this
        later.</p>

      <p>We’re thankful for feedback and support from community members including TODO: Add these.</p>

      <h2>Appendix</h2>

      <h3>Methods</h3>

      <h4>Identifying Related Embeddings with Cosine Similarity Heatmaps</h4>

      <p>Even though we had fairly strong prior expectations over which sets of vocabulary items were likely to be
        related to each other, we needed a method for pulling out these groups of embeddings in an unbiased fashion.
        They are more useful when clustered so we use <a
          href="https://www.google.com/url?q=https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html&sa=D&source=editors&ust=1697689300260402&usg=AOvVaw1raJ8i4bEzq-nLfzaC6l_S">scikit-learn</a>
        to perform agglomerative clustering based on single linkage with euclidean distance. This is just a fancy method
        for finding similar groups of tokens.</p>

      <p>This works quite effectively for these embeddings but likely would be insufficient in the case of a language
        model. Only the largest underlying feature (if any) would determine the nearest points and so you would struggle
        to retrieve meaningful clusters. A probing strategy or use of <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&sa=D&source=editors&ust=1697689300260614&usg=AOvVaw2oW9zs7QCNFL6lBeyFuw6v">sparse</a>
        <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition&sa=D&source=editors&ust=1697689300260718&usg=AOvVaw3QXc69BIpjNg28D_6KRBd-">autoencoders</a>
        to find features followed by measuring token similarity with those features might be better in that case.
      </p>

      <h4>Principal Component Analysis on a Subset of Embeddings for Feature Identification</h4>

      <p>Clustering heatmaps aren’t useful for understanding geometry unless they have very few vectors so we make use
        of Principal Component Analysis for this instead. Principal Component Analysis is a statistical technique which
        constructs an orthonormal basis from the directions of maximum variance within a vector space.</p>

      <p>It turns out that PCA is very useful for showing feature geometry in this case, for the following reasons:</p>
      <ol>
        <li>Dimensionality Reduction. Embedded vectors are very high dimensional but PCA can show us if the space can be
          understood in terms of many fewer dimensions.</li>
        <li>Quantifying variance explained. We use the percent variance explained to suggest the quality of the
          approximation achieved by the first 2 or 3 principal component vectors as in other recent mechanistic
          interpretability <a
            href="https://www.google.com/url?q=https://arxiv.org/abs/2307.09458&sa=D&source=editors&ust=1697689300261196&usg=AOvVaw0uhoNWs6eIzXSWPgwuchhe">analyses</a>.
        </li>
      </ol>

      <p>There are two issues with PCA:</p>
      <ol>
        <li>It’s not obvious that the directions found by PCA on subsets of embedding space correspond to meaningful
          features by default. We can address this by biasing the directions it finds by taking sets of embeddings
          performing PCA on them only. This makes the direction of maximal variance more likely to correspond to the
          linear representation of the semantic feature which is shared by these embeddings.</li>
        <li>Vectors produced by PCA are orthogonal which may not be true of the underlying features. For this reason it
          might make sense to interpret any features we think we find with caution.</li>
      </ol>

      <p>In order to interpret principal components, we project them onto the embedding space for relevant channels
        (mainly keys/balls) and then show the resulting scores arranged in a grid with the same shape as the
        observations generated by the MiniGrid Environment. It’s possible to interpret these by making reference to the
        positions where different vocabulary items sit and which concepts they represent.</p>

      <h3>Interpreting Feature Directions with Feature Maps</h3>

      <p>Once we have a direction which we believe corresponds to a meaningful feature, we’re able to take the cosine
        similarity between this direction and every element of embedding space. Since the embedding space is inherently
        structured as a 7×7 grid with 20 channels, we can simply look at the embeddings for the relevant channels (keys,
        and balls). This is similar to a convolution with height/width and as many channels as the embedding dimension.
      </p>

      <p>Feature maps are similar to the heat maps produced by Neel in his <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world&sa=D&source=editors&ust=1697689300261817&usg=AOvVaw1F4HGjokijlh-4NEQjfEyL">investigation</a>
        into OthelloGPT, using probe directions where we used embeddings and the residual stream where we use our
        feature.</p>

      <h3>Validating Identified Features by Embedding Arithmetic</h3>

      <p>In order to test whether a linear feature representation corresponds to a feature, we could intervene directly
        on the feature, removing or adding it from the observation token, but we can also simply add or subtract
        vocabulary items which contain that feature.</p>

      <p>Our method is similar to the <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2308.10248&sa=D&source=editors&ust=1697689300262124&usg=AOvVaw1pxx8vBfDTI-M6sOyN0zvt">activation
          addition</a> technique which operates on the residual stream at a token position, but works at the level of
        the input. If we operated directly on the hypothesised linear feature representation direction then this method
        would be similar to the causal intervention on the word model used on <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world&sa=D&source=editors&ust=1697689300262231&usg=AOvVaw2gg9VdYEYDgdjTRGcryV1o">OthelloGPT</a>
        to test whether a probe vector could be used to intervene in a transformers world representation.</p>

      <p>In order to evaluate the effect of each possible embedding arithmetic, we take the modal scenario where the
        model has walked forward 4 times and is choosing between left / right. We measure the logit difference between
        left and right in the following contexts:</p>
      <ul>
        <li>A negative control (the base-case).</li>
        <li>A positive control (the in-distribution complement).</li>
        <li>The test case (the out-of-distribution complement).</li>
      </ul>

      <p>Then for each test case we report the proportion of logit difference restored \(\frac{LD(\text{test}) -
        LD(\text{negative control})}{LD(\text{positive control}) - LD(\text{negative control})}\).</p>

      <p>This is identical to the metric we would use if evaluating the effect size of a patching experiment and while
        it hides some of the variability in the results, it also makes the trends very obvious.</p>

      <h3>Related Work</h3>

      <p><strong>Decision Transformers:</strong><a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/2106.01345.pdf&sa=D&source=editors&ust=1697689300262813&usg=AOvVaw1bVu-qY-FGPFDnFi6_p3DL">Decision
          Transformers</a> are one of <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2106.02039&sa=D&source=editors&ust=1697689300262878&usg=AOvVaw3zK38j7ngRQJ92xQElSwSW">several</a>
        methods developed to apply transformers to RL tasks. These methods are referred to as “offline” since the
        transformer learns from a corpus of recorded trajectories. Decision Transformers are conditioned to predict
        actions consistent with a given reward because they are “goal conditioned” receiving a token representing
        remaining reward to be achieved at each timestep. The decision transformer architecture is the basis for SOTA
        models developed by DeepMind including <a
          href="https://www.google.com/url?q=https://openreview.net/pdf?id%3D1ikK0kHjvj&sa=D&source=editors&ust=1697689300262990&usg=AOvVaw0Vr5AZs9uww_rTiYQAJogT">Gato</a>
        (a highly generalist agent) and <a
          href="https://www.google.com/url?q=https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent&sa=D&source=editors&ust=1697689300263086&usg=AOvVaw2ICQhPOrspbozx5jD3IZAH">Robocat</a>
        (A foundation agent for robotics).</p>

      <p><strong>GridWorld Decision Transformers:</strong>Earlier this year we studied a small <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability&sa=D&source=editors&ust=1697689300263279&usg=AOvVaw2-NpsUGtKbIDubvtbWSIAA">gridworld
          decision transformer</a> mainly via attribution and ablations. More recently, I posted details about <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/JvQWbrbPjuvw4eqxv/a-mechanistic-interpretability-analysis-of-a-gridworld-agent&sa=D&source=editors&ust=1697689300263380&usg=AOvVaw0m0jsgr5F0BslhNwOwnA0m">MemoryDT</a>,
        the model discussed in this post.</p>

      <p><strong>Circuit-Style Interpretability:</strong>A large body of <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2207.13243&sa=D&source=editors&ust=1697689300263591&usg=AOvVaw34O7EIvEb1ey3cnYRzUaLc">previous
          work</a> exists attempting to understand the inner structures of deep neural networks. Focusing on the most
        relevant work to this investigation, we attempt to find features/linear feature representations with the framing
        the <a
          href="https://www.google.com/url?q=https://distill.pub/2020/circuits/zoom-in/&sa=D&source=editors&ust=1697689300263670&usg=AOvVaw2SxG2TRG4hKdlN46UqLlSz">circuit
          style interpretability</a>. We make reference to previously documented phenomena such as <a
          href="https://www.google.com/url?q=https://distill.pub/2020/circuits/equivariance/&sa=D&source=editors&ust=1697689300263737&usg=AOvVaw1Jg7xMs-ma9lo15hToAQ_Q">equivariance</a>,
        <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2209.10652&sa=D&source=editors&ust=1697689300263806&usg=AOvVaw0XQM1K8lSL_T4z6E0wC_xe"></a>isotropic
        superposition</a> (previously “superposition”) and recently documented <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&sa=D&source=editors&ust=1697689300263890&usg=AOvVaw0Tqj-61Cdylyjv5y-e2vyX">anisotropic
          superposition</a>. Our use of PCA was inspired by its application to key/query and value subspaces in <a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/2307.09458.pdf&sa=D&source=editors&ust=1697689300263954&usg=AOvVaw0uZ5xijseEOfbY1HxupbFf">analysis</a>
        in the 70B Chinchilla Model.
      </p>

      <p><strong>Linear Representations:</strong>Linear algebraic structure has been previously <a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/1601.03764.pdf&sa=D&source=editors&ust=1697689300264180&usg=AOvVaw1M1VLU4ipjj5fMhDmMZpJ8">predicted</a>
        in word embeddings and found using techniques such as <a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/1910.03833.pdf&sa=D&source=editors&ust=1697689300264287&usg=AOvVaw2J4s0u0TiIrtNKsaNHdpsE">dictionary
          learning</a> and <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/1711.08792&sa=D&source=editors&ust=1697689300264362&usg=AOvVaw1pjlevKDLLRi9sJMmuImfB">sparse
          autoencoders</a>. Such representations can be understood as suggesting that the underlying token embedding is
        a sum of “word factors” or features.</p>

      <img src="images//image23.png" alt="Equation" style="display:block; margin:auto;">
      <p>Taken from <a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/1910.03833.pdf&sa=D&source=editors&ust=1697689300264597&usg=AOvVaw0FnUNEtWx5iHPSiSzBeSJm">Zhang
          et al 2021</a></p>

      <p>More recently, efforts have been made to find linear feature representations in the residual stream with
        techniques such as <a
          href="https://www.google.com/url?q=https://aclanthology.org/2021.deelio-1.1.pdf&sa=D&source=editors&ust=1697689300264759&usg=AOvVaw0daT23ioQdbnRrYHshmL1E">dictionary
          learning</a>, <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition&sa=D&source=editors&ust=1697689300264856&usg=AOvVaw1v7zVq6DBwbDan6XZBnPZq">sparse
          auto-encoders</a> or <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2305.01610&sa=D&source=editors&ust=1697689300264926&usg=AOvVaw3_1OvQ7FMBh3juuqXOwS6M">sparse
          linear probing</a>. What started as an attempt to understand how language models deal with polysemy (the
        property of a word/token having more than one distinct meaning), has continued as a much more ambitious attempt
        to understand how language models represent information in all layers.</p>

      <p><strong>RL Interpretability:</strong>A variety of previous investigations have applied interpretability
        techniques to models solving RL tasks. <b>Convolutional Neural Networks:</b> This includes <a
          href="https://www.google.com/url?q=https://distill.pub/2020/understanding-rl-vision/&sa=D&source=editors&ust=1697689300265159&usg=AOvVaw0ZX1L5WpjGYCVwLAKOzPIS">analysis
          of a convolutional neural network</a> solving the Procgen <a
          href="https://www.google.com/url?q=https://openai.com/research/procgen-benchmark&sa=D&source=editors&ust=1697689300265227&usg=AOvVaw21fQ7IiYFjUmS41R8keS1I">>CoinRun</a>
        task using attribution and model editing. Similarly, <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/sequences/sCGfFb5DPfjEmtEdn&sa=D&source=editors&ust=1697689300265299&usg=AOvVaw10JaMPi6W1SVnVb77sMeVn">a
          series of investigations</a> into models which solve the Procgen <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/sequences/sCGfFb5DPfjEmtEdn&sa=D&source=editors&ust=1697689300265368&usg=AOvVaw2KrBC8ErEz97QxLzZfInVg">Maze</a>
        task identified a subset of channels responsible for identifying the target location which could be retargeted
        (a limited version of <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget&sa=D&source=editors&ust=1697689300265490&usg=AOvVaw0k9hoE9sfV9T24XW5jVwt5">retargeting
          the search</a>). Transformers: An investigation by <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2210.13382&sa=D&source=editors&ust=1697689300265582&usg=AOvVaw3GFHDE26wFLsk-Xo9VIXV6">Li
          et al.</a> found evidence for a non-linear world representation in an offline-RL agent playing Othello using
        probes. It was later found that this <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2309.00941&sa=D&source=editors&ust=1697689300265647&usg=AOvVaw3KtH0Db6zxRJxgLYVpZs6o">world
          representation was linear</a> and amenable to causal interventions.</p>

      <p><strong>Antipodal Representations:</strong>Toy models of superposition were found to use antipodal directions
        to <a
          href="https://www.google.com/url?q=https://transformer-circuits.pub/2022/toy_model/index.html%23geometry-organization&sa=D&source=editors&ust=1697689300265850&usg=AOvVaw3YEHX_z2-rgVGsw9_zKLc3">represent
          anti-correlated features in opposite directions</a>. There is some evidence that and we’ve seen that language
        models also make use of <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight%23Negative&sa=D&source=editors&ust=1697689300265978&usg=AOvVaw1oP-MDUJF49rDZA-o5xPNG">antipodal
          representations</a>.</p>

      <p><strong>Adversarial Inputs:</strong><a
          href="https://www.google.com/url?q=https://arxiv.org/abs/1706.06083&sa=D&source=editors&ust=1697689300266182&usg=AOvVaw0JpC4Y09Ymzm5gbIoxsvQO">Adversarial
          examples</a> are important to both <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/kYNMXjg8Tmcq3vjM6&sa=D&source=editors&ust=1697689300266265&usg=AOvVaw3C_saHDRKJDbUs35CTwdnf">interpretability</a>
        and AI safety. A relevant debate is whether these are <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/1905.02175&sa=D&source=editors&ust=1697689300266330&usg=AOvVaw09AZgJHEmM6v3sMFSzeD0k">bugs
          or features</a> (with our work suggesting the latter), though possibly the topic should be approached with
        significant nuance.</p>

      <p><strong>Activation Additions/Steering Vectors:</strong>We discuss <a
          href="https://www.google.com/url?q=https://arxiv.org/pdf/2308.10248.pdf&sa=D&source=editors&ust=1697689300266507&usg=AOvVaw3f6mTCwl3srp2pEJaUUJJG">activation
          addition</a> as equivalent to our embedding arithmetic (due to our observation tokenization schema).
        Activation additions attempt steering language model generation underpinned by <a
          href="https://www.google.com/url?q=https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector%23Benefits_from_paired__counterbalanced_activation_additions&sa=D&source=editors&ust=1697689300266635&usg=AOvVaw0fcDeHASyBgyYASDqf5RC6">paired,
          counterbalanced</a> vectors in activation space. Similar <a
          href="https://www.google.com/url?q=https://arxiv.org/abs/2205.05124&sa=D&source=editors&ust=1697689300266706&usg=AOvVaw0RhzLPlC4OVL24t9sApp8s">steering
          <a
            href="https://www.google.com/url?q=https://arxiv.org/abs/2304.00740&sa=D&source=editors&ust=1697689300266763&usg=AOvVaw0se6VMqo4E7XkhHf0o3jY9">approaches</a>
          have been developed previously finding directions with stochastic gradient descent. Of particular note, one
          investigation used an <a
            href="https://www.google.com/url?q=https://arxiv.org/abs/2306.03341&sa=D&source=editors&ust=1697689300266827&usg=AOvVaw0EX0fqUv4xZSl9rcouk3wU">internal
            direction representing truth</a> to steer model generation.</p>

  </d-article>

  <d-appendix>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>